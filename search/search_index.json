{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":"<p>En la era digital actual, el campo de la Ingenier\u00eda en Telecomunicaciones se encuentra en constante evoluci\u00f3n, impulsado por la creciente demanda de conectividad y el acceso a datos a una escala sin precedentes. Dentro de este panorama, el Machine Learning ha surgido como una herramienta poderosa para extraer conocimientos valiosos de estos datos, transformando la manera en que interactuamos con la informaci\u00f3n y mejorando la toma de decisiones en una amplia gama de aplicaciones. En este contexto, las pr\u00e1cticas profesionales desempe\u00f1an un papel crucial al abordar desaf\u00edos complejos y relevantes que enfrenta la industria de las telecomunicaciones.</p>"},{"location":"#objetivos-generales","title":"Objetivos Generales","text":"<ul> <li> <p>Explorar y evaluar un conjunto de tecnolog\u00edas esenciales que permiten la implementaci\u00f3n exitosa de pr\u00e1cticas de Machine Learning Operations (MLOps) en el entorno universitario, haciendo hincapi\u00e9 en la reutilizaci\u00f3n, la reproducibilidad y la escalabilidad. La implementaci\u00f3n de sistemas de MLOps se ha vuelto cada vez m\u00e1s crucial, ya que proporciona la infraestructura necesaria para respaldar a los cient\u00edficos y expertos en la universidad en su b\u00fasqueda de soluciones basadas en Machine Learning. Facilita la gesti\u00f3n eficiente del ciclo de vida de los modelos, desde su desarrollo y entrenamiento hasta su despliegue y monitorizaci\u00f3n, permitiendo as\u00ed la reutilizaci\u00f3n de los recursos, la reproducibilidad de los resultados y la escalabilidad de las soluciones.</p> </li> <li> <p>El an\u00e1lisis y comparaci\u00f3n de las siguientes tecnolog\u00edas: Airflow, Kubeflow, MLFlow y DVC. Cada una de estas herramientas tiene su conjunto \u00fanico de caracter\u00edsticas y capacidades que las hacen adecuadas para diferentes escenarios y requisitos. La selecci\u00f3n de la soluci\u00f3n m\u00e1s id\u00f3nea para el Cluster de la Universidad ser\u00e1 el resultado de una evaluaci\u00f3n exhaustiva, teniendo en cuenta aspectos como la reutilizaci\u00f3n de los componentes, la reproducibilidad de los experimentos y la escalabilidad de las soluciones implementadas, permitiendo la facilidad de uso y la capacidad de gestionar eficazmente los flujos de trabajo de Machine Learning.</p> </li> <li> <p>La implementaci\u00f3n exitosa de la tecnolog\u00eda seleccionada en el Cluster de la Universidad, demostrando as\u00ed el compromiso de la instituci\u00f3n con la adopci\u00f3n de las mejores pr\u00e1cticas para los desaf\u00edos tecnol\u00f3gicos del futuro.</p> </li> </ul>"},{"location":"#objetivos-especificos","title":"Objetivos espec\u00edficos","text":"<ul> <li>Investigar y evaluar tecnolog\u00edas clave en el campo de MLOps, centr\u00e1ndose en la reutilizaci\u00f3n, reproducibilidad y escalabilidad.</li> <li>Seleccionar la herramienta m\u00e1s adecuada, entre Airflow, Kubeflow, MLFlow y DVC, para implementar pr\u00e1cticas eficientes de MLOps en el entorno universitario.</li> <li>Dise\u00f1ar e implementar una infraestructura basada en la herramienta seleccionada, que permita gestionar el ciclo de vida completo de los modelos de Machine Learning.</li> <li>Desarrollar pipelines de flujo de trabajo utilizando la herramienta seleccionada, para automatizar tareas como el entrenamiento, evaluaci\u00f3n y despliegue de modelos.</li> <li>Integrar modelos de Machine Learning en los pipelines, asegurando la correcta reproducci\u00f3n de los resultados y la reutilizaci\u00f3n de componentes.</li> <li>Realizar pruebas exhaustivas y ajustes en los pipelines implementados, garantizando su funcionamiento correcto y eficiente.</li> <li>Redactar un informe final que documente el proceso de implementaci\u00f3n, destacando la reutilizaci\u00f3n, reproducibilidad y escalabilidad logradas, as\u00ed como los beneficios obtenidos para la universidad en el \u00e1mbito de MLOps.</li> </ul>"},{"location":"02_workplan/","title":"Plan de trabajo","text":""},{"location":"02_workplan/#cronograma-de-actividades","title":"Cronograma de actividades","text":"<p>Del 27 de Septiembre al 7 de Diciembre de 2023 con 6 horas diarias en la franja horaria de 8 a 14 hs.</p> <ul> <li> <p>Actividad N\u00ba1 (A1): B\u00fasqueda y revisi\u00f3n bibliogr\u00e1fica. Contextualizaci\u00f3n. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba2 (A2):  Investigaci\u00f3n y evaluaci\u00f3n de la herramienta. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba3 (A3): Dise\u00f1o e implementaci\u00f3n de la infraestructura basada en Kubeflow . Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba4 (A4): Desarrollo de pipelines de flujo de trabajo. Duraci\u00f3n: 48 horas.</p> </li> <li> <p>Actividad N\u00ba5 (A5): Integraci\u00f3n de modelos de Machine Learning en los pipelines. Duraci\u00f3n: 42 horas.</p> </li> <li> <p>Actividad N\u00ba6 (A6): Pruebas y ajustes de los pipelines. Duraci\u00f3n: 36 horas.</p> </li> <li> <p>Actividad N\u00ba7 (A7): Redacci\u00f3n de documentaci\u00f3n e informe. Duraci\u00f3n: 58 horas.</p> </li> </ul>"},{"location":"02_workplan/#septiembre-2023","title":"Septiembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 35 28 29 30 31 1 2 3 36 4 5 6 7 8 9 10 37 11 12 13 14 15 16 17 38 18 19 20 21 22 23 24 39 25 26 27 (A1) 28 (A1) 29 (A1) 30 1"},{"location":"02_workplan/#octubre-2023","title":"Octubre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 39 25 26 27 28 29 30 1 40 2  (A1) 3 (A1) 4 (A1) 5 (A1) 6 (A2) 7 8 41 9 (A2) 10 (A2) 11 (A2) 12 (A2) 13 (A2) 14 15 42 16 (A2) 17 (A3) 18 (A3) 19 (A3) 20 (A3) 21 22 43 23 (A3) 24 (A3) 25 (A3) 26 (A4) 27 (A4) 28 29 44 30 (A4) 31 (A4) 1 2 3 4 5"},{"location":"02_workplan/#noviembre-2023","title":"Noviembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 44 30 31 1 (A4) 2 (A4) 3 (A4) 4 5 45 6 (A4) 7 (A5) 8 (A5) 9 (A5) 10 (A5) 11 12 46 13 (A5) 14 (A5) 15 (A5) 16 (A5) 17 (A6) 18 19 47 20 (A6) 21 (A6) 22 (A6) 23 (A6) 24 (A6) 25 26 48 27 (A7) 28 (A7) 29 (A7) 30 (A7) 1 2 3"},{"location":"02_workplan/#diciembre-2023","title":"Diciembre 2023","text":"Week Mon Tue Wed Thu Fri Sat Sun 48 27 28 29 30 1(A7) 2 3 49 4(A7) 5(A7) 6(A7) 7(A7) 8 9 10 50 11 12 13 14 15 16 17 51 18 19 20 21 22 23 24 52 25 26 27 28 29 30 31"},{"location":"03_cover_page/","title":"Car\u00e1tula","text":"<ul> <li> <p>T\u00edtulo del tema de la Pr\u00e1ctica: Despliegue de plataforma para desarrollo, entrenamiento e implementaci\u00f3n de modelos de machine learning en cloud computing</p> </li> <li> <p>Apellido y Nombre del alumno: Aguilera Conti, Agust\u00edn.</p> </li> <li> <p>Apellido y Nombre de los Tutores (empresa y universidad):</p> <ul> <li>Ing. Daniel Anunziata</li> <li>Dr. Ing. David Marcelo de Yong</li> </ul> </li> <li> <p>Lugar de realizaci\u00f3n de la Pr\u00e1ctica: Grupo de Investigaci\u00f3n e Innovaci\u00f3n Tecnol\u00f3gica aplicada a la Ciencia de Datos - GCID</p> </li> <li> <p>Per\u00edodo de realizaci\u00f3n de la Pr\u00e1ctica: 13/09/2023 al 07/12/2023</p> </li> <li> <p>Fecha de presentaci\u00f3n del Informe: 07/12/2023</p> </li> </ul>"},{"location":"04_abstract/","title":"Resumen","text":"<p>En este informe se detallan las tareas realizadas durante la Pr\u00e1ctica Profesional Supervisada (PPS) del estudiante Agust\u00edn Aguilera Conti, perteneciente a la carrera de Ingenier\u00eda en Telecomunicaciones. El objetivo principal es exponer las actividades esenciales, los resultados obtenidos y las conclusiones relevantes derivadas del despliegue de una plataforma para el desarrollo, entrenamiento e implementaci\u00f3n de modelos de machine learning en entornos de cloud computing.</p> <p>En la primera secci\u00f3n de este documento, se proporciona un marco te\u00f3rico que abarca toda la informaci\u00f3n y conceptos adquiridos durante la fase de b\u00fasqueda y revisi\u00f3n bibliogr\u00e1fica. A continuaci\u00f3n, se presentan en detalle las tareas ejecutadas.</p> <p>Las actividades descritas en este informe incluyen, en primer lugar, la revisi\u00f3n e investigaci\u00f3n para la selecci\u00f3n de una plataforma adecuada entre las tecnolog\u00edas open source disponibles. Se logr\u00f3 identificar la herramienta m\u00e1s apropiada para implementar pr\u00e1cticas eficientes de MLOps en el entorno universitario. Posteriormente, se procedi\u00f3 al dise\u00f1o e implementaci\u00f3n de la infraestructura necesaria para poner en funcionamiento dicha herramienta. Finalmente, se llevaron a cabo pruebas exhaustivas para garantizar el correcto funcionamiento de la plataforma, marcando as\u00ed la consecuci\u00f3n de los objetivos planteados.</p>"},{"location":"05_index/","title":"Indice de contenidos","text":"<ul> <li>Desarrollo</li> <li>Objetivos<ul> <li>Objetivos generales</li> <li>Objetivos espec\u00edficos</li> </ul> </li> <li>Descripci\u00f3n de la Empresa</li> <li>Descripci\u00f3n de las tareas realizadas<ul> <li>Contextualizaci\u00f3n: DEVOps y MLOps</li> <li>Contexto de trabajo y estado del arte</li> <li>Comparaci\u00f3n y selecci\u00f3n de herramienta</li> <li>Metodolog\u00edas \u00c1giles y GitHub</li> <li>Dise\u00f1o e implementaci\u00f3n de la infraestructura basada en la   herramienta seleccionada</li> <li>Recursos f\u00edsicos - Cluster del Laboratorio</li> <li>Virtualizaci\u00f3n - Proxmox</li> <li>Aprovisionamiento de infraestructura y software<ul> <li>Terraform</li> <li>Ansible</li> <li>Uso en conjunto - Terraform + Ansible</li> </ul> </li> <li>Orquestaci\u00f3n de contenedores - Kubernetes</li> <li>Contexto de Testing Local vs Producci\u00f3n en el Cluster del Laboratorio</li> <li>Despliegue en Cluster: Puntos Claves de Ansible, Kubernetes (k8s) y Kubeflow con Informaci\u00f3n Adicional</li> <li>Pruebas realizadas</li> </ul> </li> <li>Tareas a futuro</li> <li>Conclusi\u00f3n</li> <li>Bibliograf\u00eda</li> <li>Anexo</li> </ul>"},{"location":"06_body/","title":"Desarrollo","text":""},{"location":"06_body/#objetivos","title":"Objetivos","text":""},{"location":"06_body/#objetivo-general","title":"Objetivo General","text":"<p>Explorar, evaluar un conjunto de tecnolog\u00edas esenciales que permiten la implementaci\u00f3n exitosa de pr\u00e1cticas de Machine Learning Operations (MLOps) en el entorno universitario, haciendo hincapi\u00e9 en la reutilizaci\u00f3n, la reproducibilidad y la escalabilidad. La implementaci\u00f3n de sistemas de MLOps se ha vuelto cada vez m\u00e1s crucial, ya que proporciona la infraestructura necesaria para respaldar a los cient\u00edficos y expertos en la universidad en su b\u00fasqueda de soluciones basadas en Machine Learning. Facilita la gesti\u00f3n eficiente del ciclo de vida de los modelos, desde su desarrollo y entrenamiento hasta su despliegue y monitoreo, permitiendo as\u00ed la reutilizaci\u00f3n de los recursos, la reproducibilidad de los resultados y la escalabilidad de las soluciones.</p>"},{"location":"06_body/#objetivos-especificos","title":"Objetivos Espec\u00edficos","text":"<ul> <li> <p>Investigar y evaluar tecnolog\u00edas clave en el campo de MLOps, centr\u00e1ndose en la reutilizaci\u00f3n, reproducibilidad y escalabilidad.</p> </li> <li> <p>Seleccionar la herramienta m\u00e1s adecuada, entre las tecnolog\u00edas disponibles open source, para implementar pr\u00e1cticas eficientes de MLOps en el entorno universitario.</p> </li> <li> <p>Dise\u00f1ar e implementar la infraestructura basada en la herramienta seleccionada, que permita gestionar el ciclo de vida completo de los modelos de Machine Learning.</p> </li> <li> <p>Desarrollar pipelines de flujo de trabajo utilizando la herramienta seleccionada, para automatizar tareas como el entrenamiento, evaluaci\u00f3n y despliegue de modelos.</p> </li> <li> <p>Integrar modelos de Machine Learning en los pipelines, asegurando la correcta reproducci\u00f3n de los resultados y la reutilizaci\u00f3n de componentes.</p> </li> <li> <p>Realizar pruebas exhaustivas y ajustes en los pipelines implementados, garantizando su funcionamiento correcto y eficiente.</p> </li> <li> <p>Redactar un informe final que documente el proceso de implementaci\u00f3n, destacando la reutilizaci\u00f3n, reproducibilidad y escalabilidad logradas, as\u00ed como los beneficios obtenidos para la universidad en el \u00e1mbito de MLOps.</p> </li> </ul>"},{"location":"06_body/#descripcion-de-la-empresa","title":"Descripci\u00f3n de la Empresa","text":"<p>El Grupo de Investigaci\u00f3n en Ciencia de Datos (GCID) se sit\u00faa en la vanguardia de la investigaci\u00f3n y la innovaci\u00f3n en el campo de la Ciencia de Datos. Este grupo se caracteriza por su enfoque interdisciplinario, que abarca m\u00e9todos cient\u00edficos, procesos y sistemas dise\u00f1ados para extraer conocimiento de datos en diversas formas, desde datos estructurados hasta datos no estructurados. La Ciencia de Datos es una disciplina que unifica estad\u00edsticas, an\u00e1lisis de datos, aprendizaje autom\u00e1tico y m\u00e9todos relacionados para comprender y analizar fen\u00f3menos reales utilizando t\u00e9cnicas y teor\u00edas provenientes de diversas \u00e1reas del conocimiento. El GCID se encuentra en el epicentro de la revoluci\u00f3n tecnol\u00f3gica de la Industria 4.0, donde la acumulaci\u00f3n exponencial de datos, la inteligencia artificial y la interconexi\u00f3n masiva de sistemas y dispositivos digitales est\u00e1n transformando radicalmente la forma en que las organizaciones operan y toman decisiones. El grupo se enfoca en aprovechar estas tendencias para brindar soluciones tanto a la Universidad Nacional de R\u00edo Cuarto como a entidades p\u00fablicas y privadas a nivel nacional e internacional. Para abordar los desaf\u00edos de la Ciencia de Datos, el GCID trabaja en estrecha colaboraci\u00f3n con tecnolog\u00edas habilitadoras como el Internet de las cosas (IoT), La Industria 4.0, los sistemas ciber f\u00edsicos, la cultura maker entre otros. Estas tecnolog\u00edas proporcionan el marco necesario para la recopilaci\u00f3n, el procesamiento y el an\u00e1lisis de datos en tiempo real. El grupo reconoce que extraer informaci\u00f3n valiosa de grandes vol\u00famenes de datos es un desaf\u00edo multidisciplinario que requiere una visi\u00f3n integral. Por lo tanto, los miembros del grupo provienen de diversos campos, incluyendo matem\u00e1ticas, estad\u00edsticas, ciencia de la informaci\u00f3n y computaci\u00f3n, lo que promueve la colaboraci\u00f3n y el enfoque conjunto en la resoluci\u00f3n de problemas. El GCID se esfuerza por convertirse en un referente en la Universidad Nacional de R\u00edo Cuarto y la regi\u00f3n en temas relacionados con la Ciencia de Datos, a trav\u00e9s de la docencia, la investigaci\u00f3n, el desarrollo tecnol\u00f3gico y la transferencia de conocimiento al medio.</p>"},{"location":"06_body/#descripcion-de-las-tareas-realizadas","title":"Descripci\u00f3n de las tareas realizadas","text":""},{"location":"06_body/#contextualizacion-devops-y-mlops","title":"Contextualizaci\u00f3n: DEVOps y MLOps","text":"<p>En el contexto del proyecto, la implementaci\u00f3n de pr\u00e1cticas DevOps y MLOps se traduce en la automatizaci\u00f3n eficiente de los procesos relacionados con la infraestructura, el desarrollo de software, el entrenamiento de modelos de aprendizaje autom\u00e1tico y la implementaci\u00f3n de soluciones. Estos enfoques garantizan una entrega continua, confiable y eficiente, permitiendo a los equipos de desarrollo y operaciones colaborar de manera efectiva y proporcionar soluciones que se ajusten a los requisitos del proyecto de manera \u00e1gil y escalable. La adopci\u00f3n de DevOps y MLOps contribuye a la eficiencia, la calidad y la innovaci\u00f3n en el desarrollo y operaci\u00f3n del proyecto.</p> <p></p> <p>Con lo mencionado, se entiende que las p\u0155acticas llevada a cabo en este proyecto est\u00e1n estrictamente relacionadas con estos conceptos, entendiendo que hay un equipo de desarrolladores (cient\u00edficos) que se ver\u00e1n beneficiados por el despliegue e implementaci\u00f3n de esta herramienta que seleccionamos.</p>"},{"location":"06_body/#contexto-de-trabajo-y-estado-del-arte","title":"Contexto de trabajo y estado del arte","text":"<p>El laboratorio se realiza sobre un cluster de 3 nodos f\u00edsicos, los cuales proveen los recursos necesarios que ser\u00e1n utilizados por el hipervisor y herramienta de virtualizaci\u00f3n Proxmox para agrupar dichos recursos. Lo anterior permiti\u00f3 crear una infraestructura virtual basada en m\u00e1quinas virtuales, separando nuestras pruebas del resto de los dem\u00e1s ambientes de desarrollo. Entre las tecnolog\u00edas disponibles open soirce, para implementar las p\u0155acticas eficientes de MLOps propuestas: Kubeflow, Airflow, MLFlow y DVC, todas ellas pueden ser desplegadas sobre Kubernetes, com\u00fanmente conocido como K8s, es una plataforma de c\u00f3digo abierto dise\u00f1ada para automatizar, implementar y gestionar aplicaciones en contenedores. Su funci\u00f3n principal es orquestar contenedores, facilitando la administraci\u00f3n eficiente y escalable de aplicaciones. Ampliamente utilizado en entornos de nube y locales, Kubernetes se basa en la tecnolog\u00eda de contenedores, proporcionando un entorno aislado y port\u00e1til para aplicaciones y sus dependencias. Todo lo nombrado anteriormente se condice con los objetivos planteados para esta pr\u00e1ctica.</p> <p>Por otro lado, para la realizaci\u00f3n de esta pr\u00e1ctica se defini\u00f3 utilizar metodolof\u00edas \u00e1giles basadas en Scrum utilizando la plataforma de GitHub. Esto nos permiti\u00f3 llevar un orden y seguimiento del desglose de tareas a la hora de resolver las actividades planteada.</p>"},{"location":"06_body/#comparacion-de-herramientas","title":"Comparaci\u00f3n de herramientas","text":"<p>Kubeflow ofrece una forma escalable de entrenar y desplegar modelos en Kubernetes. Es un medio de orquestaci\u00f3n que permite que un framework de aplicaciones en la nube funcione sin problemas. Algunos de los componentes de Kubeflow son los siguientes:</p> <ul> <li> <p>Notebooks: Ofrece servicios para crear y gestionar cuadernos Jupyter interactivos en entornos corporativos. Tambi\u00e9n incluye la posibilidad de que los usuarios construyan contenedores de Notebooks o pods directamente en clusters.</p> </li> <li> <p>Entrenamiento de modelos de TensorFlow: Kubeflow viene con un \"job operator\" (1) de TensorFlow personalizado que facilita la configuraci\u00f3n y ejecuci\u00f3n del entrenamiento de modelos en Kubernetes. Kubeflow tambi\u00e9n admite otros frameworks mediante job operators a medida, pero su madurez puede variar.</p> </li> <li> <p>Pipelines: Los pipelines de Kubeflow permiten construir y gestionar flujos de trabajo de aprendizaje autom\u00e1tico de m\u00faltiples pasos que se ejecutan en contenedores Docker.</p> </li> <li> <p>Despliegue: Kubeflow ofrece varias formas de desplegar modelos en Kubernetes a trav\u00e9s de complementos externos.</p> </li> </ul> <p>MLflow es un framework de c\u00f3digo abierto para el seguimiento de todo el ciclo de aprendizaje autom\u00e1tico de principio a fin, desde la formaci\u00f3n hasta la implementaci\u00f3n. Entre las funciones que ofrece se encuentran el seguimiento de modelos, la gesti\u00f3n, el empaquetado y las transiciones centralizadas de etapas del ciclo de vida. Algunos de los componentes de MLflow son los siguientes:</p> <ul> <li> <p>Seguimiento: Mientras ejecutas tu c\u00f3digo de aprendizaje autom\u00e1tico, hay una API y una interfaz de usuario para registrar par\u00e1metros, versiones de c\u00f3digo, m\u00e9tricas y archivos de salida para que puedas visualizarlos m\u00e1s tarde.</p> </li> <li> <p>Proyecto: Proporcionan un estilo est\u00e1ndar para empaquetar c\u00f3digo de ciencia de datos reutilizable; no obstante, cada proyecto es un directorio de c\u00f3digo o un repositorio Git que utiliza un archivo descriptor para indicar las dependencias y c\u00f3mo ejecutar el c\u00f3digo.</p> </li> <li> <p>Modelos: Los modelos MLflow son un est\u00e1ndar para la distribuci\u00f3n de modelos de aprendizaje autom\u00e1tico en una variedad de sabores. Hay varias herramientas disponibles para ayudar con el despliegue de varios modelos. Cada modelo se guarda como un directorio con archivos arbitrarios y un archivo de descripci\u00f3n del modelo ML que identifica los sabores en los que se puede utilizar.</p> </li> <li> <p>Registro: Le ofrece un almac\u00e9n de modelos centralizado, una interfaz de usuario y un conjunto de API para gestionar de forma colaborativa el ciclo de vida completo de su modelo MLflow. Proporciona linaje de modelos, versiones de modelos, transiciones de etapas y anotaciones.</p> </li> </ul> <p>Airflow es una plataforma de gesti\u00f3n de flujos de trabajo de c\u00f3digo abierto creada por Airbnb en 2014 para crear, supervisar y programar mediante programaci\u00f3n los crecientes flujos de trabajo de la empresa. Algunos de los componentes de Airflow son los siguientes:</p> <ul> <li> <p>Scheduler: Supervisa las tareas y los DAG, activa los flujos de trabajo programados y env\u00eda las tareas al ejecutor para que las ejecute. Est\u00e1 dise\u00f1ado para ejecutarse como un servicio persistente en el entorno de producci\u00f3n de Airflow.</p> </li> <li> <p>Ejecutores: Son mecanismos que ejecutan instancias de tareas; pr\u00e1cticamente ejecutan todo en el planificador. Los ejecutores tienen una API com\u00fan y puede intercambiarlos en funci\u00f3n de los requisitos de su instalaci\u00f3n. S\u00f3lo puede tener configurado un ejecutor por vez.</p> </li> <li> <p>Servidor web: Una interfaz de usuario que muestra el estado de sus trabajos y le permite ver, activar y depurar DAGs (2) y tareas. Tambi\u00e9n le ayuda a interactuar con la base de datos y a leer registros del almac\u00e9n de archivos remoto.</p> </li> <li> <p>Base de datos de metadatos: La base de datos de metadatos es utilizada por el ejecutor, el servidor web y el scheduler para almacenar el estado.</p> </li> </ul> <p>Data Version Control(DVC) es un sistema de control de versiones de c\u00f3digo abierto utilizado en proyectos de aprendizaje autom\u00e1tico. Tambi\u00e9n se conoce como Git para ML. Se ocupa de las versiones de datos en lugar de las versiones de c\u00f3digo. DVC le ayuda a lidiar con grandes modelos y archivos de datos que no pueden ser manejados usando Git. Le permite almacenar informaci\u00f3n sobre las diferentes versiones de sus datos para realizar un seguimiento adecuado de los datos de ML y acceder al rendimiento de su modelo m\u00e1s tarde. Puede definir un repositorio remoto para enviar sus datos y modelos, lo que facilita la colaboraci\u00f3n entre los miembros del equipo.</p> <p>Para obtener el resultado deseado, los usuarios no tienen que recordar manualmente qu\u00e9 modelo de datos utiliza qu\u00e9 conjunto de datos y qu\u00e9 acciones se llevaron a cabo; de todo esto se encarga DVC. Consiste en un conjunto de herramientas y procesos que rastrean las versiones cambiantes de los datos y las colecciones de datos anteriores. Los repositorios de DVC contienen los archivos que est\u00e1n bajo el efecto del sistema de control de versiones. Se mantiene un estado clasificado para cada cambio que se confirma en cualquier archivo de datos.</p> <p>Luego de analizar las capacidades de cada uno de los sistemas vistos anteriormente, podemos decir que si nuestro sistema necesita tratar con m\u00faltiples tipos de flujo de trabajo, no s\u00f3lo aprendizaje autom\u00e1tico, Airflow puede ayudarnos mejor. Es un marco de orquestaci\u00f3n de flujos de trabajo maduro, con soporte para muchos operadores, adem\u00e1s del aprendizaje autom\u00e1tico.</p> <p>Si deseamos un sistema con patrones predise\u00f1ados para el aprendizaje autom\u00e1tico y que funcione a gran escala en cl\u00fasteres Kubenetes, podemos considerar Kubeflow. Muchos componentes espec\u00edficos de ML en Kubeflow pueden ahorrarnos tiempo a comparaci\u00f3n de si los hacemos con Airflow.</p> <p>Si queremos desplegar MLOps en un sistema a peque\u00f1a escala (por ejemplo, una estaci\u00f3n de trabajo, o un port\u00e1til), nos conviene elegir Airflow + MLflow, ya que elimina la necesidad de configurar y ejecutar un sistema Kubenetes, y ahorrar m\u00e1s recursos para las tareas principales.</p> <p></p> <p>Como DVC se dedica a una porci\u00f3n muy espec\u00edfica y similar a MLFlow, queda en la misma categor\u00eda que el p\u00e1rrafo anterior, no cumpliendo el ciclo completo y necesitando de la combinaci\u00f3n con otro sistema.</p> <p>En nuestro caso de aplicaci\u00f3n, como nuestro cluster ya tiene Kubernetes y consideramos que es m\u00e1s completo y abarca todo el ciclo de trabajo completo, la elecci\u00f3n ideal ser\u00eda Kubeflow, por lo que ahondaremos m\u00e1s en el mismo para poder realizar su implementaci\u00f3n.</p> <p>(1) Job operator: Es un recurso personalizado de Kubernetes que permite correr tareas de entrenamiento de TensorFlow en dicha plataforma.</p> <p>(2) DAGs: Directed Acyclic Graph, es una forma de modelar las redes neuronales en forma de nodos interconectados por flechas.</p>"},{"location":"06_body/#metodologias-agiles-y-github","title":"Metodolog\u00edas \u00e1giles y GitHub","text":"<p>En la elecci\u00f3n de la metodolog\u00eda \u00e1gil para la gesti\u00f3n de este proyecto, se opt\u00f3 por Scrum debido a su capacidad para fomentar la colaboraci\u00f3n efectiva, la entrega iterativa y la adaptabilidad a los cambios en los requisitos del cliente. Scrum, alineado con los valores y principios del Manifiesto \u00c1gil, proporciona un marco de trabajo estructurado con roles claramente definidos, destacando la importancia de la comunicaci\u00f3n constante y la entrega continua de software valioso. La divisi\u00f3n de roles, con el tutor docente asumiendo el papel de Scrum Master y el tutor externo como Product Owner, permite una distribuci\u00f3n clara de responsabilidades. Adem\u00e1s, la elecci\u00f3n de sprints semanales se alinea con la naturaleza del proyecto, facilitando una planificaci\u00f3n detallada y una entrega regular de incrementos de valor.</p> <p></p> <p>Ver anexo: Metodolog\u00edas \u00c1giles</p> <p>En cuanto al Git Workflow, se adopt\u00f3 GitHub Workflow por su simplicidad y la familiaridad con la plataforma en la que se desarrolla el proyecto. GitHub Workflow ofrece un enfoque liviano y eficiente para el control de versiones, integrando la entrega continua con la capacidad de desplegar inmediatamente en la rama master. Este enfoque se ajusta adecuadamente a la naturaleza colaborativa del desarrollo, permitiendo la creaci\u00f3n de ramas descriptivas para nuevas caracter\u00edsticas, la apertura de Pull Requests para facilitar la revisi\u00f3n y la r\u00e1pida integraci\u00f3n en la rama principal una vez que se ha confirmado la funcionalidad.</p> <p></p> <p>Ver anexo: GitHub</p> <p>En conjunto, la combinaci\u00f3n de Scrum como metodolog\u00eda \u00e1gil y GitHub Workflow como enfoque de control de versiones establece un marco s\u00f3lido para la gesti\u00f3n y desarrollo efectivos de este proyecto, enfoc\u00e1ndose en la entrega continua, la adaptabilidad y la colaboraci\u00f3n eficiente entre los diferentes actores involucrados.</p> <p>La adopci\u00f3n de esta combinaci\u00f3n demostr\u00f3 ser sumamente beneficiosa en la fase de adquisici\u00f3n de conocimientos sobre las nuevas tecnolog\u00edas a implementar. La orientaci\u00f3n hacia las tareas espec\u00edficas y los objetivos concretos proporciona una flexibilidad significativa en t\u00e9rminos de gesti\u00f3n del tiempo y resoluci\u00f3n desaf\u00edos de manera inmediata. Esto permit\u00f3 la resoluc\u00f3n \u00e1gil de problermas y el aprendizaje efectivo. Adem\u00e1s, la naturaleza \u00e1gil de este enfoque permiti\u00f3 ajustes r\u00e1pidos de objetivos, permitiendo cambios din\u00e1micos en la direcci\u00f3n del proyecto si un objetivo particular no alcanza un destino previsto o se eval\u00faa como de escasa utilidad. En conjunto, estos elementos contribuyeron a una curva de aprendizaje inicial gradual que, una vez superada, aceler\u00f3 notablemente la ejecuci\u00f3n de tareas, resultando en una alta productividad.</p> <p>Enlace al repositorio utilizado: https://github.com/danunziata/pps-agustin_conti_2023</p>"},{"location":"06_body/#diseno-e-implementacion-de-la-infraestructura-basada-en-la-herramienta-seleccionada","title":"Dise\u00f1o e implementaci\u00f3n de la infraestructura basada en la herramienta seleccionada","text":"<p>Para implementar la plataforma Kubeflow, hemos definido requisitos espec\u00edficos de infraestructura y software. Nuestra infraestructura se basa en un cluster compuesto por tres PC, donde utilizamos Proxmox para crear m\u00e1quinas virtuales con recursos espec\u00edficos y desplegar sobre ellas el software necesario.</p>"},{"location":"06_body/#recursos-fisicos-cluster-del-laboratorio","title":"Recursos f\u00edsicos - Cluster del Laboratorio","text":"<p>Este conjunto consiste en tres PC con recursos significativos que suman un total de 96 CORES de CPU, 96 GB de Memoria RAM, y 3 TB de almacenamiento, aunque no cuentan con GPU por el momento.</p>"},{"location":"06_body/#virtualizacion-proxmox","title":"Virtualizaci\u00f3n - Proxmox","text":"<p>Proxmox Virtual Environment, conocido como Proxmox, es una plataforma de virtualizaci\u00f3n de c\u00f3digo abierto. Facilita la administraci\u00f3n y despliegue de m\u00e1quinas virtuales (VM) y contenedores en un entorno integrado. Proxmox utiliza KVM para VMs y LXC para contenedores, permitiendo ejecutar ambos en la misma plataforma. Su interfaz web, Proxmox Virtual Environment (PVE), simplifica la gesti\u00f3n de recursos f\u00edsicos, creaci\u00f3n de VMs, copias de seguridad, y m\u00e1s. Proxmox destaca por la gesti\u00f3n centralizada de recursos, diversas opciones de almacenamiento, administraci\u00f3n de cl\u00fasteres para alta disponibilidad, seguridad, y escalabilidad.</p> <p></p> <p>Seleccionamos Proxmox para nuestra plataforma debido a sus m\u00faltiples prestaciones y funcionalidades que se alinean con nuestros requisitos.</p>"},{"location":"06_body/#aprovisionamiento-de-infraestructura-y-software","title":"Aprovisionamiento de infraestructura y software","text":"<p>En sistemas y tecnolog\u00eda, el aprovisionamiento configura y suministra recursos, como servidores y redes, para satisfacer las necesidades de una aplicaci\u00f3n. Este proceso, crucial para entornos de nube, se realiza mediante herramientas como Terraform. Terraform es una herramienta de infraestructura como c\u00f3digo (IaC) que automatiza y estandariza la creaci\u00f3n y configuraci\u00f3n de recursos de infraestructura de manera consistente y repetible.</p> <p>Terraform</p> <p>Terraform, desarrollado por HashiCorp, es esencial para la creaci\u00f3n y configuraci\u00f3n de infraestructura como c\u00f3digo. Utiliza declaraciones en archivos de configuraci\u00f3n para definir recursos y dependencias, adoptando un enfoque declarativo y proporcionando una gesti\u00f3n eficiente. Terraform es compatible con diversos proveedores de infraestructura, permitiendo su uso en entornos locales o en la nube.</p> <p></p> <p>Para nuestro caso, necesitamos proveer m\u00e1quinas virtuales sobre Proxmox, gracias a Terraform pudimos hacer una configuraci\u00f3n previa considerando las necesidades de recursos, pudiendo definir la cantidad de n\u00facleos, memoria RAM y almacenamiento, adem\u00e1s de configurar algunas funcionalidades escenciales para la identificaci\u00f3n (como el nombre de las m\u00e1quinas e ID), la administraci\u00f3n (claves SSH) y la conectividad (IPs), permitiendo as\u00ed, tener una base para luego ser\u00e1 usada para el aprovisionamiento de software.</p> <p>Ver anexo: Terraform</p> <p>Ansible</p> <p>Ansible, tambi\u00e9n de c\u00f3digo abierto, se centra en la automatizaci\u00f3n y gesti\u00f3n de configuraciones de software. A diferencia de Terraform, trabaja en sistemas ya aprovisionados y se destaca por ser \"sin agente\". Utiliza Playbooks y Roles para describir tareas y configuraciones en sistemas remotos a trav\u00e9s de SSH o WinRM.</p> <p>Seleccionamos Ansible para configurar m\u00e1quinas virtuales, crear un Cluster de Kubernetes y aprovisionar archivos necesarios para Kubeflow. Su enfoque en la eficiencia y consistencia en la instalaci\u00f3n de software result\u00f3 crucial. Se estructuraron roles y playbooks para una m\u00e1xima automatizaci\u00f3n y reproductibilidad.</p> <p></p> <p>Ansible fue de vital utilidad para configurar nuestras m\u00e1quinas virtuales, crear un Cluster de Kubernetes funcional con dichas m\u00e1quinas y aprovisionar con los diferentes archivos que ser\u00e1n necesarios para la instalaci\u00f3n de nuestra plataforma, Kubeflow. En el proceso se defini\u00f3 la estructura de archivos requerida para el funcionamiento de esta herramienta, donde lo m\u00e1s importante a destacar son las \"variables de grupo\" que nos permitieron hacer un \"plantillado\" por usuario, de manera de asegurar la reproducibilidad de nuestro experimentos. Por otro lado se definieron \"roles\" que constan de archivos que contienen instructivos o tareas. Cada rol se puede asignar a \"tareas generales\" distintas, por lo que es f\u00e1cil separar el trabajo en secciones m\u00e1s peque\u00f1as que garanticen el entendimiento y por el otro lado, nos permitan aprovisionar \"por secci\u00f3n\" o \"paso a paso\" si es que lo deseamos gracias al uso del \"tagging\" (etiquetado) de dichos roles.</p> <p>Los roles fueron seccionados en diferentes \"etapas de configuraci\u00f3n\" donde se realizaron, entre otras tareas, la limpieza de viejos archivos de la m\u00e1quina host, la instalaci\u00f3n de multiples dependencias, la configuraci\u00f3n del kernel de las m\u00e1quinas remotas, la instalaci\u00f3n de Kubernetes, la inicializaci\u00f3n del cluster y la uni\u00f3n de los workers a dicho cluster y la descarga e instalaci\u00f3n de los requerimientos de Kubeflow.</p> <p>Estos roles y sus tareas fueron ordenados y asignados a los diferentes hosts gracias a a un \"playbook\" principal a partir del cual se seguia el orden de intalaci\u00f3n, utilizando los hosts definidos en el \"inventario\".</p> <p>Es de mucha importancia aclarar que durante todo el desarrollo del aprovisionamiento se busc\u00f3 la m\u00e1xima automatizaci\u00f3n a la hora de la creaci\u00f3n tanto del entorno de pruebas, como del aprovisionamiento en s\u00ed.</p> <p>Ver anexo: Ansible</p> <p>Uso en conjunto - Terraform + Ansible</p> <p>La combinaci\u00f3n de Terraform y Ansible es poderosa para gestionar integralmente infraestructura y software. Terraform se emplea para el aprovisionamiento inicial de recursos, mientras que Ansible configura y administra servidores y aplicaciones. Juntos, orquestan la implementaci\u00f3n de aplicaciones, gestionan actualizaciones y cambios en la infraestructura, permiten la escalabilidad autom\u00e1tica y garantizan la coherencia de la configuraci\u00f3n continua. La integraci\u00f3n de ambas herramientas abarca desde la creaci\u00f3n de infraestructura hasta la administraci\u00f3n completa del ciclo de vida del sistema, ofreciendo un enfoque completo y automatizado.</p>"},{"location":"06_body/#orquestacion-de-contenedores-kubernetes","title":"Orquestaci\u00f3n de contenedores - Kubernetes","text":"<p>La investigaci\u00f3n para nuestra plataforma Kubeflow involucr\u00f3 la comparaci\u00f3n entre dos opciones, Kubernetes (K8s) y k0s. Optamos por Kubernetes debido a su exhaustiva funcionalidad y nuestra comprensi\u00f3n profunda de sus diversos componentes. Estos componentes incluyen nodos maestros y de trabajo, despliegues, pods, servicios, y configuraci\u00f3n declarativa. Al implementar un Cluster de Kubernetes espec\u00edfico para Kubeflow, obtenemos escalabilidad seg\u00fan las necesidades de recursos, redundancia para mantener la continuidad del servicio y la capacidad de gestionar autom\u00e1ticamente la asignaci\u00f3n de contenedores.</p> <p>Adem\u00e1s, se destacan tres elementos esenciales en la instalaci\u00f3n de Kubernetes para Kubeflow: la Container Runtime Interface (CRI) con la elecci\u00f3n de CRI-O por su ligereza, la Container Network Interface (CNI) para la configuraci\u00f3n din\u00e1mica de recursos de red, y la StorageClass (SC) para proporcionar vol\u00famenes de almacenamiento de manera din\u00e1mica y personalizada mediante Rancher. Estos elementos garantizan una implementaci\u00f3n robusta y eficiente del entorno Kubernetes necesario para Kubeflow.</p> <p></p> <p>Ver anexo: Kubernetes</p>"},{"location":"06_body/#contexto-de-testing-local-vs-produccion-en-el-cluster-del-laboratorio","title":"Contexto de Testing Local vs Producci\u00f3n en el Cluster del Laboratorio","text":"<p>Para llevar a cabo pr\u00e1cticas y pruebas en un entorno local antes de la implementaci\u00f3n en el cluster del laboratorio, se dise\u00f1\u00f3 una estrategia espec\u00edfica. Dado que no se contaba con acceso remoto al cluster, se opt\u00f3 por construir una l\u00f3gica de pruebas locales. En este contexto, se eligi\u00f3 Vagrant como la herramienta principal para emular la infraestructura del cluster. Vagrant facilit\u00f3 la creaci\u00f3n y gesti\u00f3n de m\u00e1quinas virtuales en el entorno local del desarrollador, permitiendo probar de manera eficiente las configuraciones y roles de Ansible dise\u00f1ados para el aprovisionamiento. Esta aproximaci\u00f3n asegur\u00f3 un proceso de desarrollo ordenado y reproducible.</p> <p>Vagrant, en este escenario, ofreci\u00f3 un entorno aislado donde el desarrollador pudo desplegar y validar las herramientas necesarias, especialmente Ansible, para la instalaci\u00f3n autom\u00e1tica y configuraci\u00f3n de los componentes esenciales, como Kubernetes (k8s) y Kubeflow. Esta pr\u00e1ctica no solo simplific\u00f3 la detecci\u00f3n y correcci\u00f3n de posibles problemas, sino que tambi\u00e9n garantiz\u00f3 la consistencia y la reproducibilidad del entorno de prueba. La capacidad de Vagrant para crear y destruir f\u00e1cilmente las m\u00e1quinas virtuales facilit\u00f3 la iteraci\u00f3n en el desarrollo de roles de Ansible y en las primeras pruebas de despliegue de k8s y Kubeflow. Con esta estrategia, se estableci\u00f3 una base s\u00f3lida antes de la implementaci\u00f3n final en el cluster del laboratorio, asegurando que las configuraciones y herramientas estuvieran preparadas para su despliegue en un entorno de producci\u00f3n m\u00e1s complejo.</p> <p></p> <p>Ver anexo: Vagrant</p>"},{"location":"06_body/#despliegue-en-cluster-puntos-claves-de-ansible-kubernetes-k8s-y-kubeflow-con-informacion-adicional","title":"Despliegue en Cluster: Puntos Claves de Ansible, Kubernetes (k8s) y Kubeflow con Informaci\u00f3n Adicional","text":"<p>Durante el proceso de despliegue en el cluster del laboratorio, se identificaron puntos clave en las herramientas utilizadas, especialmente en Ansible, Kubernetes (k8s) y Kubeflow.</p> <p>En el caso de Ansible, la configuraci\u00f3n y ejecuci\u00f3n de roles resultaron esenciales. La estructuraci\u00f3n adecuada de roles y playbooks permiti\u00f3 una instalaci\u00f3n ordenada y modular de componentes en el cluster. La capacidad de Ansible para trabajar con hosts definidos en inventarios facilit\u00f3 la asignaci\u00f3n de tareas a nodos espec\u00edficos, optimizando as\u00ed el proceso de despliegue. Adem\u00e1s, la incorporaci\u00f3n de etiquetas (tags) en roles posibilit\u00f3 una ejecuci\u00f3n selectiva, permitiendo enfoques paso a paso y la repetici\u00f3n eficiente de tareas espec\u00edficas. Un elemento crucial fue la implementaci\u00f3n del \"plantillado\" o \"creaci\u00f3n de perfiles\", que result\u00f3 vital para asegurar diferentes configuraciones personalizadas para cada usuario o prueba, permitiendo seleccionar la infraestructura deseada.</p> <p>En el \u00e1mbito de Kubernetes (k8s), la comprensi\u00f3n de la arquitectura y la interacci\u00f3n de los componentes fue crucial. La definici\u00f3n de pods, servicios y despliegues, junto con la correcta configuraci\u00f3n de nodos maestros y de trabajo, asegur\u00f3 un despliegue estable y eficiente. La implementaci\u00f3n de pr\u00e1cticas de seguridad, como la autenticaci\u00f3n y autorizaci\u00f3n de usuarios, tambi\u00e9n se integr\u00f3 para fortalecer la infraestructura del cluster. Durante este proceso, la capacidad de levantar el servicio de Dashboard en Kubernetes fue de vital importancia, proporcionando una visualizaci\u00f3n m\u00e1s sencilla de las operaciones durante las pruebas y la instalaci\u00f3n de Kubeflow. Asimismo, la atenci\u00f3n meticulosa a las versiones de Kubelet, Kubectl, Kubeadm, CNI y CRI, junto con el aprendizaje y aplicaci\u00f3n de comandos Kubectl, resultaron fundamentales para resolver problemas y optimizar el rendimiento del cluster.</p> <p></p> <p>En la secci\u00f3n de Kubeflow, la instalaci\u00f3n represent\u00f3 un reto significativo debido a su estricta adherencia a versiones tanto en requisitos como en m\u00e9todos de instalaci\u00f3n. Se logr\u00f3 ejecutar exitosamente un ejemplo de entrenamiento de un modelo de clasificaci\u00f3n MNIST, mientras se exploraba la funcionalidad del Dashboard de Kubeflow en la creaci\u00f3n de Notebooks Servers. Este proceso implic\u00f3 una investigaci\u00f3n exhaustiva para encontrar las versiones compatibles y adaptarse a las rigurosas especificaciones de Kubeflow. Estos desaf\u00edos subrayan la necesidad de una cuidadosa planificaci\u00f3n y consideraci\u00f3n de versiones en proyectos similares.</p> <p></p> <p>Ver anexo: Instalaci\u00f3n de Kubeflow</p>"},{"location":"06_body/#pruebas-realizadas","title":"Pruebas realizadas","text":"<p>En el marco de las pruebas realizadas con Kubeflow, se llev\u00f3 a cabo un ejemplo con TensorFlow para resaltar la eficacia de esta plataforma en entornos de ciencia de datos e inteligencia artificial.</p> <p></p> <p>Kubeflow demostr\u00f3 ser una herramienta valiosa al posibilitar la creaci\u00f3n de perfiles y servidores de Notebooks, lo que permiti\u00f3 a los cient\u00edficos realizar diversas pruebas de manera eficiente y organizada.</p> <p>Durante estas pruebas, se exploraron los conceptos de \"pipelines\" en Kubeflow, que son flujos de trabajo automatizados que integran diversas tareas en un solo proceso.</p> <p>Por otro lado, para las pruebas con los pipelines se investig\u00f3 sobre Elyra, un editor de pipelines en JupyterLab, podr\u00eda desempe\u00f1ar un papel crucial al proporcionar una interfaz gr\u00e1fica para ensamblar pipelines a partir de cuadernos Jupyter, scripts en Python o R, y c\u00f3digo preempaquetado, todo ello sin necesidad de escribir c\u00f3digo manualmente. Aunque no se pudo poner a prueba directamente en este escenario espec\u00edfico, se percibe como una herramienta que puede facilitar significativamente el trabajo de los cient\u00edficos al proporcionar flexibilidad y eficiencia en el dise\u00f1o, ejecuci\u00f3n y personalizaci\u00f3n de pipelines.</p> <p></p> <p>Elyra tambi\u00e9n ofrece la capacidad de ejecutar estos pipelines en entornos remotos, donde Kubeflow Pipelines o Apache Airflow est\u00e1 desplegado. Al ejecutar un pipeline, Elyra genera los artefactos necesarios para el entorno de ejecuci\u00f3n objetivo y desencadena su ejecuci\u00f3n. Esto podr\u00eda ser una caracter\u00edstica valiosa para los cient\u00edficos que buscan una manera colaborativa y reproducible de llevar a cabo sus investigaciones.</p> <p>En resumen, Kubeflow, junto con la potencial integraci\u00f3n de Elyra, se vislumbra como una combinaci\u00f3n poderosa para dise\u00f1ar, ejecutar y personalizar pipelines de manera eficiente en entornos de ciencia de datos. Esto proporcionar\u00eda a los cient\u00edficos las herramientas necesarias para realizar investigaciones de manera efectiva y colaborativa.</p> <p>Ver anexo: Correr Ejemplo</p>"},{"location":"06_body/#tareas-a-futuro","title":"Tareas a futuro","text":"<p>En la continuaci\u00f3n de este proyecto de pr\u00e1cticas, se vislumbran diversos caminos para mejorar a\u00fan m\u00e1s la eficiencia y la robustez de la implementaci\u00f3n. Uno de los aspectos cruciales a abordar ser\u00eda la selecci\u00f3n de modelos a desplegar, permitiendo la construcci\u00f3n de pipelines m\u00e1s optimizados y especializados. Este enfoque facilitar\u00eda la adaptabilidad a diferentes escenarios y requerimientos, contribuyendo a una implementaci\u00f3n m\u00e1s precisa y eficiente de modelos de aprendizaje autom\u00e1tico.</p> <p>La seguridad es otra \u00e1rea de vital importancia que merece una atenci\u00f3n especial en los trabajos futuros. Fortalecer las medidas de seguridad en todos los niveles del sistema garantizar\u00eda la protecci\u00f3n de datos sensibles y la integridad de los modelos, crucial en entornos donde la confidencialidad y la privacidad son prioridades. La implementaci\u00f3n de protocolos de seguridad robustos ser\u00eda esencial para mitigar posibles riesgos y asegurar un despliegue confiable de los flujos de trabajo de machine learning.</p> <p>Asimismo, se plantea la necesidad de abordar la generaci\u00f3n de los backups y la recuperaci\u00f3n de datos en futuras etapas del proyecto. Establecer procedimientos s\u00f3lidos para realizar copias de seguridad de modelos, datos y configuraciones cr\u00edticas permitir\u00eda una r\u00e1pida recuperaci\u00f3n en caso de fallos inesperados o p\u00e9rdida de informaci\u00f3n. Esta estrategia no solo contribuir\u00eda a la continuidad operativa sino que tambi\u00e9n respaldar\u00eda la integridad de los proyectos de machine learning a largo plazo.</p> <p>Finalmente, la administraci\u00f3n de perfiles se posiciona como un aspecto clave para mejorar la colaboraci\u00f3n y la organizaci\u00f3n dentro del entorno de desarrollo. La capacidad de gestionar perfiles de manera eficiente, asignar roles y permisos de manera adecuada, facilitar\u00eda la colaboraci\u00f3n entre cient\u00edficos de datos y desarrolladores, promoviendo un flujo de trabajo armonioso y colaborativo. Integrar herramientas que simplifiquen esta administraci\u00f3n ser\u00eda esencial para optimizar la cooperaci\u00f3n en el proyecto y mejorar la productividad del equipo.</p>"},{"location":"07_conclusion/","title":"Conclusi\u00f3n","text":"<p>Se concluye que se lograron satisfactoriamente los objetivos planteados en el proceso de implementaci\u00f3n de las pr\u00e1cticas. La utilizaci\u00f3n de herramientas como Vagrant para la virtualizaci\u00f3n y prueba local de roles de Ansible demostr\u00f3 ser beneficiosa, permitiendo a los desarrolladores realizar configuraciones y pruebas de manera eficiente.</p> <p>La incorporaci\u00f3n de Kubernetes como plataforma de orquestaci\u00f3n de contenedores en la implementaci\u00f3n de Kubeflow destaca la orientaci\u00f3n hacia soluciones modernas y escalables. La creaci\u00f3n de un cl\u00faster de Kubernetes con CRI-O como interfaz de contenedores, CNI para la configuraci\u00f3n din\u00e1mica de red, y StorageClass para la gesti\u00f3n din\u00e1mica de almacenamiento evidencian una arquitectura bien estructurada.</p> <p>En el ejemplo espec\u00edfico de Kubeflow, la instalaci\u00f3n se divide en pasos claros, desde el aprovisionamiento de la infraestructura hasta la instalaci\u00f3n manual de los manifestos proporcionados. Esta metodolog\u00eda asegura un despliegue controlado y la replicabilidad del entorno, tanto en el desarrollo como en el entorno de producci\u00f3n.</p> <p>El uso de Terraform para el aprovisionamiento de infraestructura con Proxmox demuestra un enfoque integral en la automatizaci\u00f3n, facilitando la creaci\u00f3n y gesti\u00f3n de nodos de manera eficiente.</p> <p>Adem\u00e1s, la estructura detallada de la configuraci\u00f3n, como la definici\u00f3n de perfiles en archivos YAML, proporciona flexibilidad y facilita la personalizaci\u00f3n seg\u00fan las necesidades del proyecto. La inclusi\u00f3n de consideraciones de seguridad, como la configuraci\u00f3n de claves SSH y la administraci\u00f3n de usuarios, muestra un compromiso con las mejores pr\u00e1cticas.</p> <p>En resumen, la adopci\u00f3n de tecnolog\u00edas y pr\u00e1cticas modernas, junto con la atenci\u00f3n a detalles como la configuraci\u00f3n de red, recursos de hardware y seguridad, reflejan un enfoque integral y bien planificado en el desarrollo e implementaci\u00f3n de las pr\u00e1cticas. Este enfoque contribuye a la eficiencia, escalabilidad y replicabilidad de las soluciones propuestas.</p>"},{"location":"08_bibliography/","title":"Bibliograf\u00eda","text":"<ul> <li> <p>How to Manage Releases with Semantic Versioning and Git Tags</p> </li> <li> <p>4 branching workflows for Git</p> </li> <li> <p>GitHub flow - GitHub Docs</p> </li> <li> <p>Vagrant ssh key pair - DevopsRoles.com top 1.</p> </li> <li> <p>Install and Specify a Box | Vagrant | HashiCorp Developer.</p> </li> <li> <p>Introduction to DVC and MLflow for Experiment tracking. Valohai.</p> </li> <li> <p>A Comprehensive Comparison Between Kubeflow and Airflow. Valohai.</p> </li> <li> <p>A Comprehensive Comparison Between Kubeflow and MLflow. Valohai.</p> </li> <li> <p>Airflow, MLflow or Kubeflow for MLOps?. AICurious.</p> </li> <li> <p>Kubernetes Setup Using Ansible and Vagrant</p> </li> <li> <p>GitHub - AudelDiaz - kubeadm-cluster</p> </li> <li> <p>Ansible Best Practicas - Directory Layout</p> </li> <li> <p>Creaci\u00f3n de Local StorageClass y Local PersistentVolume</p> </li> <li> <p>Fix Bug HTTP for Notebook creation - \"Could not find CSRF cookie XSRF-TOKEN\"</p> </li> <li> <p>Tensorflow - Basic classification: Classify images of clothing</p> </li> <li> <p>Levantar Kubeflow con Minikube</p> </li> </ul>"},{"location":"09_attachments/","title":"Anexo","text":""},{"location":"09_attachments/#metodologias-agiles","title":"Metodolog\u00edas \u00e1giles","text":""},{"location":"09_attachments/#manifiesto-agil","title":"Manifiesto \u00e1gil","text":"<p>El Manifiesto \u00c1gil es un documento que establece los valores y principios fundamentales para el desarrollo \u00e1gil de software. Fue creado en 2001 por un grupo de expertos en desarrollo de software que buscaban alternativas m\u00e1s flexibles y eficientes a los enfoques tradicionales de gesti\u00f3n de proyectos.</p>"},{"location":"09_attachments/#valores-del-manifiesto-agil","title":"Valores del Manifiesto \u00c1gil","text":"<ol> <li>Individuos e interacciones sobre procesos y herramientas: Se enfoca en la importancia de las personas y la comunicaci\u00f3n efectiva en el desarrollo de software.</li> <li>Software funcionando sobre documentaci\u00f3n extensiva: Prioriza la entrega de software funcional y utilizable por encima de una documentaci\u00f3n exhaustiva.</li> <li>Colaboraci\u00f3n con el cliente sobre negociaci\u00f3n contractual: Destaca la importancia de la colaboraci\u00f3n continua con el cliente para adaptarse a los cambios y requisitos emergentes.</li> <li>Responder a cambios sobre seguir un plan: Aboga por la flexibilidad y la capacidad de adaptarse a cambios en los requisitos, incluso en etapas avanzadas del desarrollo.</li> </ol>"},{"location":"09_attachments/#principios-del-manifiesto-agil","title":"Principios del Manifiesto \u00c1gil","text":"<ol> <li>La m\u00e1s alta prioridad es satisfacer al cliente mediante la entrega temprana y continua de software valioso.</li> <li>Aceptar cambios en los requisitos, incluso en etapas tard\u00edas del desarrollo.</li> <li>Entregar software funcional con frecuencia, con preferencia a intervalos cortos.</li> <li>Colaborar con clientes y usuarios a lo largo del proyecto.</li> <li>Construir proyectos alrededor de individuos motivados, d\u00e1ndoles el entorno y el apoyo que necesitan y confiando en ellos para que hagan el trabajo.</li> <li>El m\u00e9todo m\u00e1s eficiente y efectivo de comunicaci\u00f3n es la conversaci\u00f3n cara a cara.</li> <li>El software funcional es la principal medida de progreso.**</li> <li>Mantenerte enfocado en la simplicidad, maximizando la cantidad de trabajo no realizado.</li> <li>Las mejores arquitecturas, requisitos y dise\u00f1os surgen de equipos autoorganizados.</li> <li>A intervalos regulares, el equipo reflexiona sobre c\u00f3mo ser m\u00e1s efectivo y ajusta su comportamiento en consecuencia.</li> </ol> <p>Hoy en d\u00eda, el Manifiesto \u00c1gil sigue siendo un marco de referencia influyente para el desarrollo de software. Las metodolog\u00edas \u00e1giles como Scrum, Kanban y XP (eXtreme Programming) se basan en estos valores y principios. Las organizaciones adoptan enfoques \u00e1giles para mejorar la flexibilidad, la capacidad de respuesta a cambios y la entrega continua de software de alta calidad. Adem\u00e1s, la cultura \u00e1gil ha trascendido el \u00e1mbito del desarrollo de software y se ha extendido a otras \u00e1reas como la gesti\u00f3n de proyectos, el marketing y la gesti\u00f3n empresarial.</p>"},{"location":"09_attachments/#scrum-roles-y-responsabilidades","title":"Scrum: Roles y responsabilidades","text":"<p>Scrum es un marco de trabajo \u00e1gil que se utiliza com\u00fanmente en el desarrollo de software para gestionar proyectos de manera iterativa e incremental. Los roles en Scrum son esenciales para la colaboraci\u00f3n y la entrega efectiva de productos. Los roles principales en Scrum y sus responsabilidades son:</p> <ol> <li>Product Owner (Due\u00f1o del Producto):<ul> <li>Responsabilidades:<ul> <li>Define la visi\u00f3n del producto.</li> <li>Prioriza el backlog del producto.</li> <li>Asegura que el equipo est\u00e9 trabajando en las caracter\u00edsticas m\u00e1s valiosas y prioritarias.</li> <li>Toma decisiones sobre el alcance y las caracter\u00edsticas del producto.</li> </ul> </li> </ul> </li> <li>Scrum Master (Facilitador del Proceso):<ul> <li>Responsabilidades:<ul> <li>Garantiza que el equipo Scrum siga las pr\u00e1cticas y reglas de Scrum.</li> <li>Facilita las reuniones del equipo, como las reuniones de planificaci\u00f3n, revisi\u00f3n y retrospectiva.</li> <li>Elimina los obst\u00e1culos que impiden el progreso del equipo.</li> <li>Ayuda a mantener un entorno de trabajo colaborativo y centrado en la entrega de valor.</li> </ul> </li> </ul> </li> <li>Equipo de Desarrollo:<ul> <li>Responsabilidades:<ul> <li>Desarrolla el producto durante los sprints.</li> <li>Colabora en la planificaci\u00f3n del sprint y define las tareas necesarias.</li> <li>Se autoorganiza para lograr los objetivos del sprint.</li> <li>Participa en las ceremonias de Scrum, como las reuniones diarias de scrum, la revisi\u00f3n y la retrospectiva.</li> </ul> </li> </ul> </li> </ol> <p>Es importante destacar que en Scrum, se fomenta la colaboraci\u00f3n y la autogesti\u00f3n del equipo. El Product Owner y el Scrum Master sirven al equipo y trabajan en conjunto para asegurar que el producto se desarrolle de manera efectiva y que se cumplan los objetivos del negocio. Adem\u00e1s, Scrum promueve la transparencia, la inspecci\u00f3n y la adaptaci\u00f3n continua, lo que permite a los equipos responder r\u00e1pidamente a los cambios en los requisitos o en el entorno del proyecto.</p> <p>En Scrum, tanto el \"Backlog\" como el \"Sprint\" son conceptos fundamentales que contribuyen al enfoque iterativo e incremental del desarrollo de software.</p> <ol> <li> <p>Backlog:</p> <p>El \"Product Backlog\" (Backlog del Producto) es una lista din\u00e1mica y priorizada de todas las funcionalidades, mejoras y tareas que podr\u00edan ser realizadas para un producto. Es responsabilidad del Product Owner mantener y gestionar este backlog. Algunas caracter\u00edsticas del Product Backlog incluyen:</p> <ul> <li>Priorizaci\u00f3n: Las \u00edtems del backlog est\u00e1n ordenados por prioridad, con las caracter\u00edsticas m\u00e1s importantes o de mayor valor para el cliente en la parte superior.</li> <li>Flexibilidad: El backlog es flexible y puede cambiar con el tiempo para adaptarse a las necesidades cambiantes del negocio o del cliente.</li> <li>Detalles: Los elementos del backlog no necesitan estar detallados en exceso. Los detalles se refinan a medida que los elementos se acercan al tope del backlog y se preparan para ser incluidos en un sprint.</li> <li>Sprint:</li> </ul> <p>Un \"Sprint\" es una unidad de tiempo fija y corta durante la cual se realiza un trabajo espec\u00edfico y se produce una versi\u00f3n potencialmente entregable del producto. Los sprints en Scrum generalmente tienen una duraci\u00f3n de dos a cuatro semanas. Algunas caracter\u00edsticas clave del Sprint incluyen:</p> <ul> <li>Objetivo del Sprint: Antes de comenzar un Sprint, el equipo selecciona elementos del Product Backlog para incluir en el Sprint Backlog, que es la lista de elementos que se comprometen a completar durante el sprint.</li> <li>Iterativo e Incremental: El trabajo se realiza en ciclos iterativos, y al final de cada Sprint, se produce un incremento potencialmente entregable del producto.</li> <li>Reuniones: Durante el sprint, el equipo se re\u00fane diariamente en la \"Daily Scrum\" para revisar el progreso y planificar el trabajo del d\u00eda.</li> <li>Revisi\u00f3n y Retrospectiva: Al final del Sprint, se llevan a cabo la \"Sprint Review\" para presentar el trabajo completado y obtener retroalimentaci\u00f3n, y la \"Sprint Retrospective\" para analizar el proceso y mejorar en futuros sprints.</li> </ul> </li> </ol> <p>La combinaci\u00f3n del Backlog y los Sprints permite a los equipos Scrum mantener un enfoque \u00e1gil y responder r\u00e1pidamente a los cambios en los requisitos del cliente o del negocio, al tiempo que entrega de manera regular incrementos de valor al producto.</p> <p>Para nuestro caso, el tutor docente actuar\u00eda como Scrum Master, el tutor externo actuar\u00eda de Product Owner y el equipo de desarrollo estar\u00eda conformado por el alumno. Adem\u00e1s hemos definido los sprints de una semana, y tomaremos tareas del backlog que se ha conformado a partir del plan de trabajo de esta PPS.</p>"},{"location":"09_attachments/#cultura-devops","title":"Cultura DevOps","text":"<p>La cultura DevOps es una filosof\u00eda y pr\u00e1ctica que promueve la colaboraci\u00f3n estrecha y continua entre los equipos de desarrollo (Dev) y operaciones (Ops) en el ciclo de vida del desarrollo de software. El objetivo principal es superar las barreras tradicionales entre estos dos departamentos para lograr una entrega de software m\u00e1s r\u00e1pida, eficiente y confiable.</p>"},{"location":"09_attachments/#principios-y-valores-de-la-cultura-devops","title":"Principios y Valores de la Cultura DevOps","text":"<ol> <li>Colaboraci\u00f3n y Comunicaci\u00f3n:<ul> <li>Principio: Fomentar la colaboraci\u00f3n abierta y una comunicaci\u00f3n efectiva entre los equipos de desarrollo y operaciones.</li> </ul> </li> <li>Automatizaci\u00f3n:<ul> <li>Principio: Automatizar tanto como sea posible los procesos de desarrollo, prueba y despliegue para mejorar la eficiencia y reducir errores.</li> </ul> </li> <li>Entrega Continua:<ul> <li>Principio: Buscar la entrega continua de software, permitiendo versiones peque\u00f1as y frecuentes en lugar de despliegues masivos y menos frecuentes.</li> </ul> </li> <li>Monitoreo y Retroalimentaci\u00f3n:<ul> <li>Principio: Implementar sistemas de monitoreo para obtener retroalimentaci\u00f3n r\u00e1pida sobre el rendimiento y la calidad del software en producci\u00f3n.</li> </ul> </li> <li>Responsabilidad Compartida:<ul> <li>Principio: Fomentar una mentalidad de responsabilidad compartida entre los equipos de desarrollo y operaciones en todo el ciclo de vida del software.</li> </ul> </li> </ol>"},{"location":"09_attachments/#como-se-utiliza-hoy-en-dia","title":"C\u00f3mo se utiliza hoy en d\u00eda","text":"<p>La cultura DevOps se implementa mediante la adopci\u00f3n de pr\u00e1cticas y herramientas espec\u00edficas, como:</p> <ul> <li>Despliegue Continuo (Continuous Deployment): Automatizaci\u00f3n del proceso de liberaci\u00f3n de software para que nuevas versiones puedan ser implementadas de manera r\u00e1pida y segura.</li> <li>Infraestructura como C\u00f3digo (Infrastructure as Code - IaC): Definir y gestionar la infraestructura de manera automatizada, trat\u00e1ndola como c\u00f3digo, lo que facilita la reproducibilidad y escalabilidad.</li> <li>Integraci\u00f3n Continua (Continuous Integration): Integrar el c\u00f3digo de los desarrolladores en un repositorio compartido varias veces al d\u00eda, lo que facilita la detecci\u00f3n temprana de errores.</li> <li>Monitoreo Continuo y An\u00e1lisis de Logs: Utilizar herramientas para monitorear el rendimiento en tiempo real y analizar los registros para identificar problemas y \u00e1reas de mejora.</li> </ul> <p>A modo de s\u00edntesis, la cultura DevOps busca mejorar la colaboraci\u00f3n, eficiencia y velocidad en el desarrollo y despliegue de software, y se apoya en la automatizaci\u00f3n, la entrega continua y la responsabilidad compartida entre los equipos de desarrollo y operaciones.</p>"},{"location":"09_attachments/#objetivos-smart","title":"Objetivos SMART","text":"<p>Los objetivos SMART son una metodolog\u00eda utilizada para establecer metas y objetivos de manera clara y espec\u00edfica. El acr\u00f3nimo SMART representa los criterios que deben cumplir estos objetivos:</p> <ol> <li>Espec\u00edficos (Specific): Los objetivos deben ser claros y espec\u00edficos, evitando ambig\u00fcedades y definiciones vagas.</li> <li>Medibles (Measurable): Deben incluir criterios cuantificables para evaluar el progreso y determinar cu\u00e1ndo se ha alcanzado el objetivo.</li> <li>Alcanzables (Achievable): Los objetivos deben ser realistas y alcanzables dentro del contexto y recursos disponibles.</li> <li>Relevantes (Relevant): Deben estar alineados con los objetivos generales y estrat\u00e9gicos de la organizaci\u00f3n.</li> <li>Temporizables (Time-bound): Deben tener un plazo o per\u00edodo de tiempo espec\u00edfico para su cumplimiento.</li> </ol> <p>Relacionando los objetivos SMART con Scrum en el contexto de Backlog, Sprint o Tarea:</p> <ul> <li>Backlog: Los elementos del Product Backlog en Scrum se benefician al ser definidos de manera SMART. Cada elemento debe ser espec\u00edfico en cuanto a su funcionalidad, medible en t\u00e9rminos de valor para el usuario, alcanzable dentro del alcance del proyecto, relevante para los objetivos del producto y con un plazo temporal definido.</li> <li>Sprint: Los objetivos de cada Sprint, establecidos durante la planificaci\u00f3n del Sprint, tambi\u00e9n pueden seguir la metodolog\u00eda SMART. Al ser espec\u00edficos, medibles, alcanzables, relevantes y temporizables, estos objetivos guiar\u00e1n el trabajo del equipo durante el Sprint.</li> <li>Tarea: Incluso a nivel de tareas individuales dentro de un Sprint, la aplicaci\u00f3n de la metodolog\u00eda SMART puede ser \u00fatil. Cada tarea puede ser definida de manera espec\u00edfica, medible en t\u00e9rminos de esfuerzo o resultados, alcanzable para un miembro del equipo, relevante para los objetivos del Sprint y con un plazo temporal asignado.</li> </ul> <p>La aplicaci\u00f3n de objetivos SMART en Scrum contribuye a una mayor claridad, enfoque y medici\u00f3n del progreso en el desarrollo de software, alineando los esfuerzos del equipo con metas claras y alcanzables.</p>"},{"location":"09_attachments/#a-modo-de-sintesis","title":"A modo de s\u00edntesis","text":"<p>Las metodolog\u00edas \u00e1giles, encabezadas por el Manifiesto \u00c1gil, han transformado la forma en que se aborda el desarrollo de software al promover valores como la flexibilidad, la colaboraci\u00f3n y la entrega continua de valor al cliente. SCRUM, una de las metodolog\u00edas \u00e1giles m\u00e1s populares, opera bajo los principios del Manifiesto \u00c1gil y estructura el desarrollo en sprints, con roles claramente definidos y un enfoque en la transparencia y adaptabilidad. La cultura DevOps, por otro lado, se alinea con los principios \u00e1giles al fomentar la colaboraci\u00f3n estrecha entre los equipos de desarrollo y operaciones, buscando la automatizaci\u00f3n y la entrega continua. En este contexto, los objetivos SMART se integran como una metodolog\u00eda clave para establecer metas claras, medibles y alcanzables, proporcionando un marco estructurado que puede aplicarse tanto a la gesti\u00f3n del backlog en SCRUM como a los objetivos espec\u00edficos de cada sprint. La combinaci\u00f3n de metodolog\u00edas \u00e1giles, SCRUM, la cultura DevOps y objetivos SMART crea un entorno de desarrollo flexible, colaborativo y orientado a resultados, permitiendo a los equipos adaptarse r\u00e1pidamente a los cambios, mejorar continuamente y cumplir con los objetivos estrat\u00e9gicos de la organizaci\u00f3n.</p>"},{"location":"09_attachments/#github","title":"GitHub","text":""},{"location":"09_attachments/#semantic-releases","title":"Semantic Releases","text":""},{"location":"09_attachments/#que-es-una-release","title":"\u00bfQu\u00e9 es una \u2018release\u2019?","text":"<p>Una release es empaquetar cualquier cambio en el c\u00f3digo y enviarlo a producci\u00f3n. Por ejemplo, un cambio de nuestra p\u00e1gina web que vaya al p\u00fablico y no a nuestra etapa de desarrollo.</p> <p>El manejo de estas releases puede ser un poco complicado, especialmente si no seguimos un cierto standard. Por eso es que usamos \u2018semantic versioning\u2019 con git tags para manejar de manera f\u00e1cil nuestras releases.</p>"},{"location":"09_attachments/#que-es-el-semantic-versioning","title":"\u00bfQu\u00e9 es el \u2018semantic versioning\u2019?","text":"<p>El semantic versioning es s\u00f3lo un esqueman num\u00e9rico, es una pr\u00e1ctica est\u00e1ndar de la industria del software que sirve para indicar el \u201cgrado de cambios\u201d que se han hecho desde la release de producci\u00f3n anterior. Todos usan semantic versioning, desde Git, hasta Firefox y los SO como iOS.</p>"},{"location":"09_attachments/#que-estructura-tiene-la-semantic-versioning","title":"\u00bfQu\u00e9 estructura tiene la semantic versioning?","text":"<p>Tiene 3 partes:</p> <pre><code>MAJOR.MINOR.PATCH\n</code></pre> <p>Cada una de las partes indica algo diferente cuando incrementa:</p> <ul> <li> <p>Major: Cuando vamos de 1.0.0 a 2.0.0 indicamos que cambiamos de manera disruptiva, incluimos cambios que no sean compatibles hacia atr\u00e1s, etc. Por ejemplo, remover c\u00f3digo que ya no sirve para incluir una reestructuraci\u00f3n completa de la arquitectura de nuestra aplicaci\u00f3n.</p> </li> <li> <p>Minor: Cuando vamos de 1.0.1 a 1.1.0 indicamos que cambiamos funcionalidades, pero que estos cambios son compatibles hacia atr\u00e1s. Puede ser el cambio de una funcionalidad, la actualizaci\u00f3n de una, el agregado de otra.</p> </li> <li> <p>Patch:  Cuando vamos de 1.0.1 a 1.0.2 indicamos arreglos de bugs y actualizaciones triviales.</p> </li> </ul>"},{"location":"09_attachments/#premisas-del-semantic-versioning","title":"Premisas del semantic versioning","text":"<ul> <li>Una vez hecha una release, la versi\u00f3n no puede ser cambiada</li> <li>Si nos olvidamos algo no podemos \u201cretaggear\u201d una versi\u00f3n, estos deber\u00edan entrar en una nueva release.</li> <li>Somos responsables de checkear reiteradamente la versi\u00f3n actual antes de hacer un release.</li> </ul>"},{"location":"09_attachments/#git-tagging-que-es-un-tag","title":"Git Tagging \u00bfQu\u00e9 es un Tag?","text":"<p>Es una manera de agregar un marcador o marker a un commit para indicar que es importante de alguna manera en nuestro repositorio. Hay dos diferentes tipos de GitTags:</p> <ul> <li> <p>Lightweigh tags: Un puntero con nombre b\u00e1sico para un commit.</p> <pre><code>&gt; git tag &lt;tag-name&gt; [commit]\n</code></pre> </li> <li> <p>Annotated tags: Un objeto completo en la database de git verificado, contiene informaci\u00f3n de el tag, tiene un mensaje de taggeo (tagging message) y puede ser firmada y verificada con GNU Privacy Guard (GPG). Esta \u00faltima es la que se recomienda usar.</p> <pre><code>&gt; git tag -a &lt;tag-name&gt; -m\"&lt;annotation&gt;\" [commit]\n</code></pre> </li> </ul> <p>Tanto el semantic versioning como el GitTagging van de la mano, podemos agregar un commit taggeando la versi\u00f3n correspondiente.</p>"},{"location":"09_attachments/#semantic-versioning-annotated-tags-semantic-releases","title":"Semantic versioning + Annotated Tags = Semantic Releases","text":"<p>Nos permite tener commits marcados en nuestro repositorio de git con una versi\u00f3n espec\u00edfica. Los beneficios de esto en un repositorio de git son:</p> <ul> <li>Le da significado a los cambios importantes en nuestro repositorio.</li> <li>Comunica el \u201cgrado de cambio\u201d entre los diferentes tags.</li> <li>Vemos de manera directa el historial de tracking de los cambios realizados.</li> </ul>"},{"location":"09_attachments/#por-que-plataformas-o-herramientas-esta-soportado-esto","title":"\u00bfPor qu\u00e9 plataformas o herramientas est\u00e1 soportado esto?","text":"<ul> <li>Diferentes interfaces de Git, como Git Kraken o GitHub Desktop.</li> <li>Diferentes herramientas de automatizaci\u00f3n como Circle CI, Bitbucket, Travis, etc.</li> </ul>"},{"location":"09_attachments/#como-creo-las-semantic-git-releases","title":"\u00bfC\u00f3mo creo las 'Semantic Git Releases'?","text":"<p>Es un proceso que consiste en 3 pasos:</p> <ol> <li>Crear un annotated tag<ol> <li>Usar semantic versioning para el nombre del tag</li> <li>Brindar una annotation</li> </ol> </li> <li>Pushear el tag al repositorio remoto</li> <li>Insertar los pasos de deployment ac\u00e1</li> </ol> <p>Crear una un semantic release tag usando la consola:</p> <pre><code>&gt; git tag -a v1.0.0 -m \"release 1.0.0\"\n&gt; git push &lt;remote&gt; v1.0.0\n</code></pre>"},{"location":"09_attachments/#release-notes","title":"Release Notes","text":"<p>Tenemos que evitar las anotaciones m\u00ednimas. Se recomienda, para cada tipo de release:</p> <ul> <li>Patch: Lista de los bug fixes</li> <li>Minor: Lista de cambios, detalles de uso.</li> <li>Major: Lista de elementos removidos, lista de cosas agregadas, proceso de actualizaci\u00f3n.</li> </ul> <p>Tomar una lista de los mensajes de los commits entre releases:</p> <pre><code>git log --pretty=format:%s &lt;last release&gt;... HEAD --no-merges\n\ngit tag -a &lt;tag-name&gt; -m\"$(git log --pretty=format:%s &lt;last release&gt;... HEAD --no-merges)\"\n</code></pre>"},{"location":"09_attachments/#como-automatizo-la-generacion-de-los-tags","title":"\u00bfC\u00f3mo automatizo la generaci\u00f3n de los tags?","text":"<ul> <li>Puedo buscar en el mercado por alguna herramienta de automatizaci\u00f3n.</li> <li>Crear un script de bash por nosotros mismos para ayudarnos a automatizar los pasos repetitivos.</li> </ul>"},{"location":"09_attachments/#git-workflow","title":"Git Workflow","text":"<p>Los Git Workflows son metodolog\u00edas de trabajo para los usuarios de de Git.  </p>"},{"location":"09_attachments/#git-flow","title":"Git Flow","text":"<p>Es el workflow m\u00e1s conocido, basado en dos branches principales que son perpetuas, con vida infinita. Estas son:</p> <ul> <li>master: Tiene el c\u00f3digo de producci\u00f3n. Todo el c\u00f3digo de desarrollo es \u2018mergeado\u2019 dentro de la branch master en alg\u00fan momento.</li> <li>develop:  Contiene el c\u00f3digo de pre-producci\u00f3n. Cuando las modificacioens o nuevas caracter\u00edsticas est\u00e9n finalizadas, se \u2018mergean\u2019 en la branch develop.</li> </ul> <p>Durante el ciclo de desarrollo, una variedad de ramas de soporte son utilizadas:</p> <ul> <li>feature-*: Usada para desarrollar nuevas caracter\u00edsitcas que vendr\u00e1n en las futuras releases. Deber\u00eda desprenderse de la rama develop y mergearse en la rama develop.</li> <li>hotfix-: Son necesarias para actuar inmediatamente ante un estado indeseado de la branch master. Deber\u00eda desprenderse de la branch master y mergearse tanto en m\u00e1ster como en develop.</li> <li>release-*: Son la preparaci\u00f3n de una nueva release de producci\u00f3n. Permiten que haya menos bugs que arreglar y la preparaci\u00f3n de la metadata para la release. Debe desprenderse de la rama develop y debe ser mergeada en la rama master y develop.</li> </ul>"},{"location":"09_attachments/#github-flow","title":"GitHub Flow","text":"<p>Es un workflow liviano creado por GitHub y se basa en 6 principios:</p> <ol> <li>Todo en la rama master es deployable.</li> <li>Para trabajar en algo nuevo, creamos una rama desde la master con un nombre descriptivo.</li> <li>Hacemos commit a esa rama localmente y regularmente hacemos push del trabajo a la misma rama en remoto.</li> <li>Cuando necesitamos feedback o creemos que es necesario mergear, abrimos un Pull Request (PR).</li> <li>Despues de que alguien haya revisado y firmado la nueva caracter\u00edstica, se puede hacer merge en la master.</li> <li>Una vez hecho el merge y pusheado a la rama master, debemos deployar inmediatamente.</li> </ol>"},{"location":"09_attachments/#gitlab-flow","title":"GitLab Flow","text":"<p>Es un workflow creado por GitLab. Combina un desarrollo dirigido por las funcionalidades (caracteristicas) y con ramas de funcionalidades con un trackeo de problemas.</p> <p>La mayor diferencia con GitHub Flow es el ambiente de las ramas que tenemos en GitLab Flow (staging y production) porque ser\u00e1 un proyecto que no puede deployarse en producci\u00f3n cada vez que hacemos un merge de una nueva feature branch. Se basa en 11 principios:</p> <ol> <li>Usa feature branches, no commits directos a master.</li> <li>Prueba todos los commits, no solo los de la master.</li> <li>Corre todos los test en todos los commits.</li> <li>Hacer revisi\u00f3n de codigo antes de hacer el merge en master.</li> <li>Los deployments son autom\u00e1ticos, basados en las branches o tags.</li> <li>Los tags son configurados por el usuario, no por el CI.</li> <li>Las releases son basadas en tags.</li> <li>Los commits ya pusheados nunca son rebasados.</li> <li>Todos comienzan por master y apuntan a master.</li> <li>Corregir bugs en master primero, release branches segundo.</li> <li>Los commits reflejan la intenci\u00f3n.</li> </ol>"},{"location":"09_attachments/#cual-elegimos","title":"\u00bfCual elegimos?","text":"<p>Por simplicidad y por la plataforma en la que estamos trabajando el workflow m\u00e1s conveniente ser\u00e1 GitHub Workflow.</p>"},{"location":"09_attachments/#vagrant","title":"Vagrant","text":"<p>Vagrant es una herramienta que pod\u00e9s usar para crear y gestionar entornos de desarrollo virtualizados de manera f\u00e1cil y reproducible. Su uso t\u00edpico es facilitar la creaci\u00f3n de m\u00e1quinas virtuales con configuraciones espec\u00edficas para el desarrollo de proyectos.</p> <p>Para comenzar un proyecto de Vagrant en el directorio /vagrant, el usuario puede seguir estos pasos:</p> <ol> <li>Instalaci\u00f3n de Vagrant: Antes que nada, necesit\u00e1s instalar Vagrant en tu m\u00e1quina. Esto se puede hacer descargando el instalador desde el sitio oficial y siguiendo las instrucciones.</li> <li>Creaci\u00f3n de un archivo Vagrantfile: En el directorio donde quer\u00e9s iniciar el proyecto, cre\u00e1 un archivo llamado <code>Vagrantfile</code>. Este archivo contendr\u00e1 la configuraci\u00f3n de la m\u00e1quina virtual, como el sistema operativo, la cantidad de memoria RAM, etc.</li> <li>Configuraci\u00f3n del Vagrantfile: Dentro del Vagrantfile, especific\u00e1 la configuraci\u00f3n deseada. Por ejemplo, pod\u00e9s elegir un sistema operativo base, asignar recursos como CPU y RAM, y configurar la red.</li> <li>Inicializaci\u00f3n de la m\u00e1quina virtual: Ejecut\u00e1 el comando <code>vagrant up</code> en el directorio donde se encuentra el Vagrantfile. Este comando crear\u00e1 y provisionar\u00e1 la m\u00e1quina virtual seg\u00fan la configuraci\u00f3n especificada.</li> <li>Acceso a la m\u00e1quina virtual: Utiliz\u00e1 el comando <code>vagrant ssh</code> para acceder a la m\u00e1quina virtual reci\u00e9n creada. Esto abrir\u00e1 una conexi\u00f3n SSH a la m\u00e1quina virtual.</li> </ol> <p>Algunas ventajas clave de utilizar Vagrant para levantar m\u00faltiples entornos de desarrollo son:</p> <ul> <li>Reproducibilidad: Con Vagrant, se puede garantizar que todos los miembros del equipo tengan exactamente el mismo entorno de desarrollo, evitando problemas de compatibilidad.</li> <li>Portabilidad: Los entornos Vagrant son independientes de la m\u00e1quina host, lo que significa que pod\u00e9s compartir el mismo entorno de desarrollo en diferentes sistemas operativos.</li> <li>Aislamiento: Cada proyecto puede tener su propio entorno virtualizado, evitando conflictos entre dependencias y facilitando la gesti\u00f3n de versiones de software.</li> <li>Eficiencia en el uso de recursos: Vagrant permite ejecutar varias m\u00e1quinas virtuales de manera eficiente, lo que es \u00fatil para simular entornos complejos, como redes privadas virtuales (VPNs) o arquitecturas de microservicios.</li> </ul>"},{"location":"09_attachments/#ejemplo-de-una-vagrantfile","title":"Ejemplo de una Vagrantfile","text":"<p>Crearemos un entorno de 3 m\u00e1quinas, una master y 2 nodos a los cuales les aplicaremos configuraciones generales y particulares a cad auno. Importante aclarar que usaremos una imagen distinta a la vista antes, en este caso ser\u00e1 <code>ubuntu/trusty64</code>.</p> <p>\u00a1Importante! Para poder configurar cierta red privada deberemos crear o modificar el archivo <code>/etc/vbox/networks.conf</code> a\u00f1adiendo la red de la siguiente manera:</p> <pre><code>sudo su\necho \"* 0.0.0.0/0\" &gt; networks.conf\n</code></pre> <p>Primero vamos a crear las claves p\u00fablicas y privadas para las conexiones SSH:</p> <p>Creamos nuestra propia clave p\u00fablica y privada con <code>ssh-keygen</code>, procuramos no poner passphrase para que no se la solicite a las VMs a la hora de iniciarlas.</p> <pre><code># ~/.ssh/\n&gt; ssh-keygen -t rsa -b 4096\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/aagustin/.ssh/id_rsa): vagrant_key\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in vagrant_key\nYour public key has been saved in vagrant_key.pub\nThe key fingerprint is:\nSHA256:K4v2o7EKOLfjCMLk7zVD4v234234c06peueU aagustin@hp-agustin\nThe key's randomart image is:\n+---[RSA 4096]----+\n|                 |\n|                 |\n|                 |\n|                 |\n| . . . .S     .  |\n|= . 3 o ..   o.. |\n|*o.+.*.... +.++. |\n|o=o.B4==  . *++  |\n|..=*+*=++ .+o. E |\n+----[SHA256]-----+\n</code></pre> <p>Ahora si, escribimos el archivo Vagrantfile:</p> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\n\nVagrant.configure(\"2\") do |config|\n\n  # Image configuration (for all vm's)\n  config.vm.box = \"bento/ubuntu-22.04\"\n  config.vm.box_version = \"202309.08.0\"\n\n  # SSH (for all vm's)\n  config.ssh.insert_key = false\n  config.ssh.forward_agent = true  \n  config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]     \n  config.vm.provision \"file\", source: \"/home/aagustin/.ssh/vagrant_key.pub\", destination: \"/home/vagrant/.ssh/authorized_keys\"\n\n  # Declaring master node and defining it like a primary machine\n  config.vm.define \"master\", primary: true do |master|\n\n    # Resources (provider)\n    master.vm.provider \"virtualbox\" do |vb|\n      vb.gui = false\n      vb.name = \"trusty64-master\"\n      vb.memory = \"2048\"\n      vb.cpus = \"2\"\n    end\n\n    # Configure synced folder\n    #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/master/\", \"/home/vagrant/\"\n\n    # Network configuration\n    master.vm.network \"public_network\",\n      bridge:\"wlo1\",\n      ip: \"192.168.102.102\",\n      netmask: \"255.255.255.0\"\n\n    master.vm.network \"private_network\",\n      ip: \"192.168.55.2\",\n      netmask: \"255.255.255.0\",\n      auto_config: false\n\n    master.vm.network \"forwarded_port\",\n      guest: 80,\n      host: 31002\n\n    # SSH\n    master.ssh.host = \"127.0.0.2\"\n    master.vm.network \"forwarded_port\",\n      guest: 22,\n      host: 2222,\n      host_ip:\"0.0.0.0\",\n      id: \"ssh\",\n      auto_correct: true\n\n    # Provisioning message\n    master.vm.provision \"shell\",\n      inline: \"echo Hello master\"\n  end\n\n  # Declaring secondary nodes (iteratively)\n  (1..2).each do |i|\n    config.vm.define \"node-#{i}\" do |node|\n\n      # Resources (provider)\n      node.vm.provider \"virtualbox\" do |vb|\n        vb.gui = false\n        vb.name = \"trusty64-node-#{i}\"\n        vb.memory = \"2048\"\n        vb.cpus = \"2\"\n      end\n\n      # Configure synced folder\n      #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/node-#{i}/\", \"/home/vagrant/\"\n\n      # Network configuration\n      node.vm.network \"public_network\",\n        bridge:\"wlo1\",\n        ip: \"192.168.102.10#{2+i}\",\n        netmask: \"255.255.255.0\"\n\n      node.vm.network \"private_network\",\n        ip: \"192.168.55.#{2+i}\",\n        netmask: \"255.255.255.0\",\n        auto_config: false\n\n      node.vm.network \"forwarded_port\",\n        guest: 80,\n        host: 31002+i\n\n      # SSH\n      node.ssh.host = \"127.0.0.#{2+i}\"\n      node.vm.network \"forwarded_port\",\n        guest: 22,\n        host: 2222+i,\n        host_ip:\"0.0.0.0\",\n        id: \"ssh\",\n        auto_correct: true\n\n      # Provisioning message\n      node.vm.provision \"shell\", inline: \"echo Hello node-#{i}\"\n    end      \n\n  end\n\nend\n</code></pre> <p>Notar que en la l\u00ednea <code>config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]</code> ponemos dos opciones, lo que logramos con esto es que use primero la por default y luego de lograda la conexi\u00f3n ya configura la clave p\u00fablica, entonces podemos acceder con la clave propia la pr\u00f3xima vez. Adem\u00e1s es importante que est\u00e9 desactivada la generaci\u00f3n por defecto de nuevas claves as\u00ed se usa la gen\u00e9rica, por eso tenemo la l\u00ednea <code>config.ssh.insert_key = false</code>.</p> <p>Levantamos nuestra configuraci\u00f3n:</p> <p>Podemos hacer <code>vagrant up</code> y por ultimo veremos el estado de estas con:</p> <pre><code>$ vagrant status\nCurrent machine states:\n\nmaster                    running (virtualbox)\nnode-1                    running (virtualbox)\nnode-2                    running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <p>Nos conectamos con las VPCs:</p> <p>Podemos acceder con SSH mediante:</p> <pre><code>ssh -p [puerto-vpc] vagrant@[ip-vpc] -i [ubicacion-priv-key]\n</code></pre> <p>Una vez hecha la conexi\u00f3n SSH, podemos ver la configuraci\u00f3n de la red que le hemos establecido a dicha m\u00e1quina virtual:</p> <pre><code>vagrant@vagrant:~$ ip -brief -c a\nlo               UNKNOWN        127.0.0.1/8 ::1/128 \neth0             UP             10.0.2.15/24 metric 100 fe80::a00:27ff:fe3b:cf90/64 \neth1             UP             192.168.102.102/24 fe80::a00:27ff:fe3b:4c8d/64 \neth2             UP             192.168.55.2/24 fe80::a00:27ff:fef5:3997/64\n</code></pre> <p>Levantamos un servicio de prueba:</p> <p>Podemos checkear el correcto funcionamiento de la IP p\u00fablica y el port forwarding levantando un servicio con python en el puerto 80 de nuestra vPc:</p> <pre><code>sudo python3 -m http.server 80\n</code></pre> <p>Y luego, podemos acceder desde el navegador de la m\u00e1quina host o cualquier navegador de cualquier dispositivo que est\u00e9 conectado a la misma red local:</p> <p></p> <p></p>"},{"location":"09_attachments/#ansible","title":"Ansible","text":""},{"location":"09_attachments/#aprovisionar-con-ansible-instalacion-y-conexion-del-host-con-el-servidor","title":"Aprovisionar con Ansible - Instalaci\u00f3n y conexi\u00f3n del host con el servidor","text":"<ol> <li>Instalamos Ansible en Ubuntu de la m\u00e1quina host:</li> </ol> <pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo apt-add-repository ppa:ansible/ansible\nsudo apt update\nsudo apt install ansible\n</code></pre> <ol> <li>Nos dirigimos a la carpeta de Ansible en nuestra m\u00e1quina host:</li> </ol> <pre><code>cd /etc/ansible\n</code></pre> <ol> <li>Veremos listados los siguientes archivos y directorios:</li> </ol> <pre><code>ansible.cfg  hosts        roles/\n</code></pre> <p>Nos nos haremos una copia de hosts en formato .yaml en nuestra carpeta de trabajo:</p> <pre><code>sudo cp ./hosts ~/workdir/hosts.yaml\n</code></pre> <p>El archivo <code>hosts.yaml</code> es el inventario donde tendremos listados todos nuestros equipos que queremos controlar.</p> <ol> <li>Configuramos el inventario <code>hosts.yaml</code> para el ejemplo, agregando las siguientes lineas:</li> </ol> <pre><code># Example for PPS\nmycluster:\n  hosts:\n    master:\n      ansible_host: 127.0.0.2\n      ansible_port: 2222\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n    nodo1:\n      ansible_host: 127.0.0.3\n      ansible_port: 2223\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n    nodo2:\n      ansible_host: 127.0.0.4\n      ansible_port: 2224\n      ansible_ssh_user: vagrant\n      ansible_ssh_private_key_file: /home/aagustin/.ssh/vagrant_key\n</code></pre> <p>Lo anterior es equivalente a crear un grupo de equipos (en nuestro caso es uno solo) llamado \"mycluster\" y dentro de ese grupo definimos los hosts llamados master, nodo1 y nodo2. Adem\u00e1s agregamos un usuario de ssh y una ruta para la llave privada, comentados, que nos servir\u00e1n luego:</p> <pre><code>ansible -i hosts.yaml  all --list-hosts\n</code></pre> <p>Donde el <code>-i</code> nos sirve para indicar que queremos usar un archivo en particular de inventario, que en nuestro caso es <code>hosts.yaml</code> (importante que estemos posicionados en el directorio de ansible <code>/etc/ansible</code> o que indiquemos la ruta completa del archivo de inventario). Este comando nos devolver\u00e1 el siguiente mensaje:</p> <pre><code>$ ansible -i hosts.yaml all --list-hosts\n  hosts (3):\n    master\n    nodo1\n    nodo2\n</code></pre> <p>Ahora deberemos configurar SSH para que Ansible pueda conectarse a los nodos que manejamos, en nuestro caso, a nuestra m\u00e1quina virtual. Para configurar SSH y permitir conexiones SSH a sistemas remotos, debemos seguir estos pasos para agregar nuestra clave p\u00fablica SSH al archivo <code>authorized_keys</code> en cada sistema remoto. En nuestro caso, ya lo hemos hecho con Vagrant:</p> <ul> <li>Generar un par de claves SSH (si a\u00fan no lo hemos hecho): Si no tenemos un par de claves SSH (una p\u00fablica y una privada), ppodemos generarlas usando el comando <code>ssh-keygen</code> en la terminal del host. Si deseamos utilizar la configuraci\u00f3n predeterminada y sin contrase\u00f1a, simplemente presionamos Enter cuando se nos solicite una contrase\u00f1a. Aqu\u00ed tienes un ejemplo:</li> </ul> <pre><code>$ ssh-keygen\n\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/aagustin/.ssh/id_rsa): id_rsa_ansible\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in id_rsa_ansible\nYour public key has been saved in id_rsa_ansible.pub\n...\n</code></pre> <p>Esto generar\u00e1 un par de claves SSH en tu directorio de inicio (por defecto) en los archivos <code>id_rsa_asible</code> (clave privada) y <code>id_rsa_ansible.pub</code> (clave p\u00fablica).</p> <ul> <li>Copiar nuestra clave p\u00fablica al sistema remoto: Ahora, debes copiar nuestra clave p\u00fablica (<code>id_rsa.pub</code> por defecto) al sistema remoto. Podemos hacerlo manualmente o utilizando el comando <code>ssh-copy-id</code>. Por ejemplo usando <code>ssh-copy-id</code>:</li> </ul> <pre><code>ssh-copy-id usuario@nombre_del_sistema_remoto\n</code></pre> <p>Esto copiar\u00e1 nuestra clave p\u00fablica al sistema remoto y la agregar\u00e1 al archivo <code>~/.ssh/authorized_keys</code> en ese sistema. Debemos asegurarnos de reemplazar <code>usuario</code> con nuestro nombre de usuario y <code>nombre_del_sistema_remoto</code> con la direcci\u00f3n IP o el nombre de host del sistema remoto.</p> <pre><code>$ ssh-copy-id vagrant@127.0.0.2\nThe authenticity of host '127.0.0.2 (127.0.0.2)' can't be established.\nED25519 key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxx.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'vagrant@127.0.0.2'\"\nand check to make sure that only the key(s) you wanted were added.\n</code></pre> <ul> <li>Iniciar sesi\u00f3n en el sistema remoto con SSH: Ahora, podemos iniciar sesi\u00f3n en el sistema remoto usando SSH y se utilizar\u00e1 nuestra clave p\u00fablica para autenticarnos:</li> </ul> <pre><code>ssh usuario@nombre_del_sistema_remoto\n</code></pre> <p>Si hemos configurado todo correctamente, no deber\u00eda ser solicitado para ingresar una contrase\u00f1a. En su lugar, se utilizar\u00e1 tu clave privada local para la autenticaci\u00f3n.</p> <pre><code> Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-83-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Mon Oct  9 02:46:20 PM UTC 2023\n\n  System load:  0.0                Processes:             140\n  Usage of /:   12.4% of 30.34GB   Users logged in:       0\n  Memory usage: 11%                IPv4 address for eth0: 10.0.2.15\n  Swap usage:   0%                 IPv4 address for eth1: 192.168.5.240\n\n\nThis system is built by the Bento project by Chef Software\nMore information can be found at https://github.com/chef/bento\nLast login: Mon Oct  9 14:32:17 2023 from 10.0.2.2\n</code></pre> <ul> <li>Repetir el proceso para otros sistemas remotos: Debes repetir estos pasos para cada sistema remoto al que deseemos acceder con SSH. Copia tu clave p\u00fablica al archivo <code>authorized_keys</code> en cada uno de esos sistemas.</li> </ul> <p>Ahora podemos corroborar la conexi\u00f3n con los mismos con un modulo de ansible llamado <code>ping</code>, para ello necesitamos unos pasos previos as\u00ed evitamos el error con el mensaje \"Permission denied (publickey,password)\" que sugiere que Ansible intent\u00f3 usar autenticaci\u00f3n mediante clave p\u00fablica SSH, pero no pudo autenticarse con \u00e9xito.</p> <p>Comando PING:</p> <p>Es momento entonces de aplicar el comando de <code>ping</code> de la siguiente manera:</p> <pre><code>ansible -i hosts.yaml all -m ping\n</code></pre> <p>Donde el <code>-m</code> indica que vamos a usar un m\u00f3dulo de Ansible.</p> <p>Si todo est\u00e1 correcto deber\u00e1 devolvernos un ping exitoso a cada una de las IP's que configuramos previamente como el siguiente:</p> <pre><code>&gt; ansible -i hosts.yaml all -m ping\nnodo1 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nnodo2 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nmaster | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>Podremos empezar a ejecutar comandos con Ansible, por ejemplo, para ver qu\u00e9 tipo de SO y qu\u00e9 versi\u00f3n tenemos instalada nos vamos a valer del siguiente comando:</p> <pre><code>ansible -i hosts.yaml nodo1 -a \"cat /etc/os-release\"\n</code></pre> <p>Donde el <code>-a</code> indica que vamos a pasar argumentos al m\u00f3dulo.</p> <p>B\u00e1sicamente lo que hacemos es enviar ese comando al nodo1, lo que nos devolver\u00e1 la lectura del archivo os-release, el cual contiene la versi\u00f3n del sistema.</p> <pre><code>nodo1 | CHANGED | rc=0 &gt;&gt;\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n</code></pre>"},{"location":"09_attachments/#aprovisionar-con-ansible-creacion-de-un-playbook","title":"Aprovisionar con Ansible - Creaci\u00f3n de un playbook","text":"<p>Ahora veremos lo que es un playbook, con el cual, haremos lo mismo que hacemos con la consola de comandos pero expresado en un archivo de instrucciones. Podremos simplificar la estructura de la siguiente manera:</p> <p>PLAYBOOK &gt; PLAYS &gt; TASKS</p> <p>El playbook es un archivo en formato <code>.yml</code> o <code>.yaml</code> . El cual, en principio crearemos en la carpeta de de trabajo <code>~/workdir/ansible/</code> para hacer m\u00e1s corta la ruta a la hora de escribir en consola, pero podr\u00eda estar en cualquier lado y la usar\u00edamos llamando a la ruta completa.</p> <pre><code>&gt; ls\nhosts.yaml  playbook.yaml\n</code></pre> <p>Permisos de usuario root en caso de ser necesario (no es el nuestro):</p> <p>Antes de poder ejecutar cualquier tarea nos debemos asegurar de tener los permisos correspondientes, para ello  vamos a configurar sudo sin contrase\u00f1a para el usuario Ansible para permitir que el usuario Ansible (el usuario con el que nos conectaamos) ejecute comandos sin requerir una contrase\u00f1a. Esto se hace editando el archivo de configuraci\u00f3n sudo <code>(/etc/sudoers)</code> en el host remoto y agregando una entrada que permita comandos espec\u00edficos sin contrase\u00f1a.</p> <p>Agregar una entrada en <code>/etc/sudoers</code> para permitir que el usuario Ansible ejecute comandos como root sin contrase\u00f1a:</p> <ul> <li>Accedemos por SSH a la m\u00e1quina de destino:</li> </ul> <pre><code>ssh vagrant@127.0.0.2\n</code></pre> <ul> <li>Abrimos el archivo sudoers para edici\u00f3n utilizando un editor de texto en el host remoto (como visudo que garantiza que no se cometan errores de sintaxis) para abrir el archivo sudoers con privilegios de superusuario:</li> </ul> <pre><code>sudo visudo\n</code></pre> <ul> <li>Agregamos la entrada para el usuario de Ansible en el archivo sudoers para permitir al usuario de Ansible ejecutar comandos sin requerir una contrase\u00f1a. La entrada debe tener el siguiente formato:</li> </ul> <pre><code># Ansible root privileges\nvagrant ALL=(ALL:ALL) NOPASSWD: ALL\n</code></pre> <ul> <li>Guardamos y cerramos.</li> </ul> <p>Esto permite que el usuario Ansible ejecute cualquier comando como root sin requerir una contrase\u00f1a. Deberemos tener en cuenta que esta opci\u00f3n tiene implicaciones de seguridad y debe usarse con precauci\u00f3n.</p> <p>Playbook</p> <p>Continuando con el playbook, la estructura de este archivo para el ejemplo de la instalaci\u00f3n de la biblioteca <code>nano</code> deber\u00e1 ser la siguiente:</p> <pre><code>---\n- name: I want to install vim # Name of the play\nhosts: mycluster #  Name of the machine or a group of machines\nbecome: yes # Adding root privileges\nbecome_method: sudo # Uses sudo to get all privileges\nbecome_user: vagrant # Once you use sudo, you become root user\ntasks:\n- name: Install vagrant # Name of the task\napt: # Name of the module\nname: vim # Library to install\nstate: latest # ersion of that library\n</code></pre> <p>Para ejecutar ese archivo de instrucciones (<code>playbook.yml</code>), usamos el siguiente comando de Ansible:</p> <pre><code>ansible-playbook -i hosts.yaml playbook.yaml\n</code></pre> <p>Lo cual nos devolver\u00e1 lo siguiente:</p> <pre><code>&gt; ansible-playbook -i hosts.yaml playbook.yaml\n\nPLAY [I want to install vim] ****************************************************************************\n\nTASK [Gathering Facts] **********************************************************************************\nok: [master]\nok: [nodo2]\nok: [nodo1]\n\nTASK [Install vim] **************************************************************************************\nok: [master]\nok: [nodo2]\nok: [nodo1]\n\nPLAY RECAP **********************************************************************************************\nmaster                     : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nnodo1                      : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nnodo2                      : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Veremos que en nuestro caso, no hubo instalaci\u00f3n de nano porque ya existia entonces, se realizaron con \u00e9xito las tareas pero no hubo cambio alguno.</p> <p>Importante que a la hora de ejecutar dichos comandos, Ansible no har\u00e1 cambios si el estado deseado ya se ha logrado previamente. Por ejemplo, en este caso que queremos instalar nano suponiendo de que no estaba instalado, cuando lo ejecutemos por primera vez detectaremos que hay cambios, pero si lo ejecutamos por segunda vez veremos que habr\u00e1 cero cambios ya que el estado deseado de tener instalado nano ya se ha cumplido.</p> <p>Si el estado deseado de la tarea fuera \"absent\" en lugar de \"latest\", cuando lo corramos de nuevo, buscar\u00e1 que dicha librer\u00eda no est\u00e9, nuevamente habr\u00e1 un cambio y ser\u00e1 la eliminaci\u00f3n de dicha librer\u00eda.</p>"},{"location":"09_attachments/#kubernetes","title":"Kubernetes","text":""},{"location":"09_attachments/#comparacion-de-diferentes-tecnologias","title":"Comparaci\u00f3n de diferentes tecnolog\u00edas","text":"<p>Deberemos explorar diferentes alternativas a la hora de ver qu\u00e9 nos conviene implementar, para ello se presentan las siguientes:</p> <p>Kubernetes (k8s): Kubernetes es la orquestaci\u00f3n de contenedores m\u00e1s robusta y ampliamente adoptada. Es ideal para despliegues a gran escala, gestionando cl\u00fasteres de contenedores con una arquitectura maestra-nodo. Ofrece una amplia gama de caracter\u00edsticas y es altamente personalizable, pero su complejidad puede ser abrumadora para proyectos m\u00e1s peque\u00f1os.</p> <p>K3s: K3s es una versi\u00f3n liviana de Kubernetes dise\u00f1ada para entornos con recursos limitados, como entornos de desarrollo local o dispositivos IoT. K3s simplifica la implementaci\u00f3n y gesti\u00f3n de Kubernetes al reducir el n\u00famero de componentes y requisitos del sistema. Esto lo hace m\u00e1s accesible para desarrolladores individuales y equipos que buscan una soluci\u00f3n m\u00e1s \u00e1gil.</p> <p>K0s: K0s es otra alternativa ligera a Kubernetes, pero se destaca por ser un cl\u00faster autocontenido y sin dependencias externas. Esto simplifica a\u00fan m\u00e1s la implementaci\u00f3n y permite ejecutar cl\u00fasteres de Kubernetes sin conexi\u00f3n a Internet. K0s es adecuado para escenarios donde la conectividad externa puede ser limitada o poco confiable.</p> <p>Minikube: Minikube es una herramienta que facilita la ejecuci\u00f3n de un cl\u00faster de Kubernetes de un solo nodo en entornos locales, como m\u00e1quinas de desarrollo. Aunque es menos adecuado para producciones a gran escala, es una opci\u00f3n pr\u00e1ctica para probar y desarrollar aplicaciones en un entorno controlado. Minikube permite a los desarrolladores experimentar con Kubernetes sin la necesidad de configuraciones complejas.</p> <p>Por el momento centraremos nuestra atenci\u00f3n en k8s y k0s, que a modo general podremos comparar rendimientos entre uno m\u00e1s completo y uno m\u00e1s simple, adem\u00e1s podremos comparar la facilidad de instalaci\u00f3n de los mismos.</p>"},{"location":"09_attachments/#k0s-version-alternativa-y-ligera-de-k8s","title":"k0s: Versi\u00f3n alternativa y ligera de K8s","text":"<p>K0s (pronunciado \"k-zeros\") es una plataforma Kubernetes ligera y autosuficiente dise\u00f1ada para ser f\u00e1cilmente desplegada en diferentes entornos, incluso aquellos con restricciones de conectividad. A diferencia de las implementaciones de Kubernetes convencionales, k0s es un cl\u00faster aut\u00f3nomo y no requiere de componentes externos para su funcionamiento. A continuaci\u00f3n, se detallan sus componentes y funcionalidades clave:</p> <ol> <li> <p>API Server (Servidor de API): El API Server de k0s es responsable de gestionar las operaciones y comunicaci\u00f3n en el cl\u00faster. Proporciona una interfaz para interactuar con los recursos de Kubernetes.</p> </li> <li> <p>Controller Manager (Administrador de Controladores): Este componente controla los controladores del sistema, que son procesos que regulan el estado del cl\u00faster y realizan acciones en respuesta a los cambios.</p> </li> <li> <p>Scheduler (Programador): El Scheduler se encarga de distribuir los pods en los nodos disponibles en funci\u00f3n de las capacidades y restricciones de estos.</p> </li> <li> <p>etcd: K0s utiliza una versi\u00f3n embebida de etcd como almacenamiento de datos distribuido para mantener la coherencia del estado del cl\u00faster.</p> </li> <li> <p>Kubelet y Kube-proxy: Estos componentes son esenciales en cualquier implementaci\u00f3n de Kubernetes. Kubelet es responsable de ejecutar contenedores en los nodos, mientras que Kube-proxy facilita la comunicaci\u00f3n de red entre los diferentes servicios.</p> </li> <li> <p>CoreDNS: Proporciona servicios de resoluci\u00f3n de nombres en el cl\u00faster, permitiendo la comunicaci\u00f3n entre los servicios por nombre en lugar de direcciones IP.</p> </li> <li> <p>Kubelet y Kube-proxy: Estos componentes son esenciales en cualquier implementaci\u00f3n de Kubernetes. Kubelet es responsable de ejecutar contenedores en los nodos, mientras que Kube-proxy facilita la comunicaci\u00f3n de red entre los diferentes servicios.</p> </li> </ol> <p>K0s es especialmente adecuado para entornos donde la conectividad a Internet es limitada o inestable, ya que puede operar de manera completamente aut\u00f3noma. Su dise\u00f1o ligero y su capacidad para funcionar como un cl\u00faster aut\u00f3nomo lo hacen apropiado para despliegues en dispositivos de borde (edge computing), entornos de desarrollo local y escenarios donde la simplicidad y la independencia de infraestructura externa son prioritarias. La facilidad de implementaci\u00f3n y la capacidad de operar en entornos variados hacen que k0s sea una opci\u00f3n atractiva para casos de uso diversos.</p> <p>Aprovisionamiento de k0s con Ansible sobre VMs de Vagrant</p> <p>Creamos la carpeta k0s, donde aplicaremos primero el siguiente comando:</p> <pre><code>vagrant init\n</code></pre> <p>Dentro de la misma carpeta deberemos tener la siguiente estructura de archivos:</p> <pre><code>k0s/\n|_ Vagrantfle\n|_ ansible/\n    |_ playbook.yaml\n    |_ inventory/\n    |    |_ inventory.yaml\n    |_ roles/\n        |_ .../\n        |_ .../\n            |_ .../\n</code></pre> <p>Ahora veremos que poner dentro de cada archivo.</p> <p>En la <code>Vagrantfile</code> deberemos tener lo siguiente:</p> <pre><code># -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\n\nVagrant.configure(\"2\") do |config|\n\n  # Image configuration (for all vm's)\n  config.vm.box = \"bento/ubuntu-22.04\"\n  config.vm.box_version = \"202309.08.0\"\n\n  # SSH (for all vm's)\n  config.ssh.insert_key = false\n  config.ssh.forward_agent = true  \n  config.ssh.private_key_path = [\"/home/aagustin/.vagrant.d/insecure_private_key\",\"/home/aagustin/.ssh/vagrant_key\"]     \n  config.vm.provision \"file\", source: \"/home/aagustin/.ssh/vagrant_key.pub\", destination: \"/home/vagrant/.ssh/authorized_keys\"\n\n  # How many VMs to create\n  VMS = 3\n\n  # We need at least:\n  # -initial_controller = must contain a single node that creates the worker and server tokens needed by the other nodes.\n  # -controller = can contain nodes that, together with the host from initial_controller form a highly available isolated control plane.\n  # -worker = must contain at least one node so that we can deploy Kubernetes objects.\n\n\n  # Declaring nodes (iteratively)\n  (1..VMS).each do |i|\n    config.vm.define \"k0s-#{i}\" do |node|\n\n      # Resources (provider)\n      node.vm.provider \"virtualbox\" do |vb|\n        vb.gui = false\n        vb.name = \"trusty64-k0s-#{i}\"\n        vb.memory = \"1024\"\n        vb.cpus = \"2\"\n      end\n\n      # Configure synced folder\n      #config.vm.synced_folder \"~/my-loc/vagrant/synced/folders/k0s-#{i}/\", \"/home/vagrant/\"\n\n      # Network configuration\n      node.vm.network \"public_network\",\n        bridge:\"wlo1\",\n        ip: \"192.168.102.20#{1+i}\",\n        netmask: \"255.255.255.0\"\n\n      node.vm.network \"private_network\",\n        ip: \"192.168.55.#{1+i}\",\n        netmask: \"255.255.255.0\",\n        virtualbox__intnet: true\n        #auto_config: false\n\n      node.vm.network \"forwarded_port\",\n        guest: 80,\n        host: 31001+i\n\n      # SSH\n      node.ssh.host = \"127.0.0.#{1+i}\"\n      node.ssh.forward_agent = true\n      node.vm.network \"forwarded_port\",\n        guest: 22,\n        host: 2221+i,\n        host_ip:\"0.0.0.0\",\n        id: \"ssh\",\n        auto_correct: true\n\n      # Provisioning message\n      node.vm.provision \"shell\", inline: \"echo Hello node-#{i}\"\n    end      \n\n  end\n\nend\n</code></pre> <p>Ejecutaremos el comando <code>vagrant up</code> para tener las m\u00e1quinas virtuales. En este caso ya supusismos que tenemos las claves privadas y p\u00fablicas creadas por lo que no deber\u00edamos tener problema. Vemos que el <code>status</code> es el siguiente:</p> <pre><code>&gt; vagrant status\nCurrent machine states:\n\nk0s-1                     running (virtualbox)\nk0s-2                     running (virtualbox)\nk0s-3                     running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <p>Ahora podemos crear el inventario de Ansible <code>inventory.yaml</code>:</p> <pre><code>---\nall:\n  children:\n    initial_controller:\n      hosts:\n        k0s-1:\n    controller:\n      hosts:\n        k0s-2:\n    worker:\n      hosts:\n        k0s-3:\n  hosts:\n    k0s-1:\n      ansible_ssh_host: 127.0.0.2\n      ansible_ssh_port: 2222\n    k0s-2:\n      ansible_ssh_host: 127.0.0.3\n      ansible_ssh_port: 2223\n    k0s-3:\n      ansible_ssh_host: 127.0.0.4\n      ansible_ssh_port: 2224\n  vars:\n    ansible_user: vagrant\n    ansible_private_key: /home/aagustin/.ssh/vagrant_key\n</code></pre> <p>Con el inventario creado, podemos controlar que tenemos los hosts bien configurados y hacer un ping para ver si tenemos conectividad:</p> <ul> <li>Listamos todos los hosts:</li> </ul> <pre><code>&gt; ansible -i ansible/inventory/inventory2.yaml --list-hosts all\n  hosts (3):\n    k0s-1\n    k0s-2\n    k0s-3\n</code></pre> <ul> <li>Ejecutamos el comando PING:</li> </ul> <pre><code>&gt; ansible -i ansible/inventory/inventory.yaml -m ping all      \nk0s-1 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nk0s-3 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nk0s-2 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre> <p>Recordar que si es la primera vez que le damos up y no indicamos ningun fingerprint deberemos aceptar que queremos ingresar sin fingerprint poniendo <code>yes</code> en la terminal cuando nos pregunte.</p> <p>Ahora nos disponemos a crear el <code>playbook.yaml</code>, con el siguiente contenido:</p> <pre><code>---\n\n- hosts: initial_controller:controller:worker\n  name: Download k0s on all nodes\n  become: yes\n  roles:\n    - role: download\n      tags: download\n    - role: prereq\n      tags: prereq\n\n- hosts: initial_controller\n  gather_facts: yes\n  become: yes\n  name: Configure initial k0s control plane node\n  roles:\n    - role: k0s/initial_controller\n      tags: init\n\n- hosts: controller\n  gather_facts: yes\n  become: yes\n  serial: 1\n  name: Configure k0s control plane nodes\n  roles:\n    - role: k0s/controller\n      tags: server\n\n- hosts: worker\n  become: yes\n  name: Configure k0s worker nodes\n  roles:\n    - role: k0s/worker\n      tags: worker\n</code></pre> <p>Este se encargar\u00e1 de llamar a las diferentes plays que ejecutan ciertas tareas en los diferentes hosts. Por eso es necesario tener el contenido de la carpeta <code>roles/</code> y sus respectivos plays.</p> <p>Una vez tenemos esto, podemos ejecutar el aprovisionamiento con Asible a los nodos creados:</p> <pre><code>ansible-playbook ansible/playbook.yaml -i ansible/inventory/inventory.yaml\n</code></pre>"},{"location":"09_attachments/#k8s-kubernetes-convencional","title":"K8s: Kubernetes convencional","text":"<p>Kubernetes (k8s) es una plataforma de c\u00f3digo abierto dise\u00f1ada para automatizar, escalar y operar aplicaciones en contenedores. Su arquitectura se basa en un modelo maestro-nodo que coordina la gesti\u00f3n de contenedores en un cl\u00faster. Aqu\u00ed se describen sus principales componentes y funcionalidades:</p> <ol> <li> <p>API Server (Servidor de API): Act\u00faa como la interfaz principal para la gesti\u00f3n y control del cl\u00faster, permitiendo la interacci\u00f3n con los objetos de Kubernetes, como pods, servicios y vol\u00famenes.</p> </li> <li> <p>etcd: Es un almac\u00e9n de datos distribuido que mantiene la configuraci\u00f3n del cl\u00faster y el estado del mismo, garantizando la coherencia entre los nodos maestros.</p> </li> <li> <p>Control Plane (Plano de Control): Incluye componentes como el API Server, etcd, el Controller Manager y el Scheduler, trabajando en conjunto para tomar decisiones sobre el estado del cl\u00faster y coordinar las acciones necesarias.</p> </li> <li> <p>Kubelet: Este agente se ejecuta en cada nodo del cl\u00faster y es responsable de asegurar que los contenedores est\u00e9n en ejecuci\u00f3n. Interact\u00faa con el API Server para recibir y ejecutar las instrucciones.</p> </li> <li> <p>Kube-proxy: Facilita la comunicaci\u00f3n de red entre los pods y gestiona las reglas de red, como el enrutamiento y el balanceo de carga.</p> </li> <li> <p>Pods: La unidad m\u00e1s peque\u00f1a en Kubernetes, que puede contener uno o m\u00e1s contenedores. Los pods comparten un espacio de red y almacenamiento, lo que facilita la comunicaci\u00f3n entre ellos.</p> </li> <li> <p>Services (Servicios): Proporcionan una abstracci\u00f3n para la comunicaci\u00f3n entre los diferentes pods, permitiendo la escalabilidad y la resiliencia de las aplicaciones.</p> </li> </ol> <p>Kubernetes es altamente vers\u00e1til y puede desplegarse en una variedad de entornos, desde infraestructuras locales hasta nubes p\u00fablicas. Es especialmente eficaz en entornos de producci\u00f3n donde la orquestaci\u00f3n y escalabilidad de contenedores son fundamentales. Kubernetes tambi\u00e9n es utilizado com\u00fanmente en entornos de desarrollo y pruebas para garantizar la coherencia entre los diferentes ciclos de vida de las aplicaciones. Su capacidad para gestionar cargas de trabajo en diversos entornos y su gran comunidad de usuarios lo hacen adecuado para una amplia gama de casos de uso.</p>"},{"location":"09_attachments/#automatizacion-de-la-implementacion-de-kubernetes-k8s-en-maquinas-vagrant-utilizando-ansible","title":"Automatizaci\u00f3n de la implementaci\u00f3n de Kubernetes (k8s) en m\u00e1quinas Vagrant utilizando Ansible","text":""},{"location":"09_attachments/#pasos-para-el-aprovisionamiento","title":"Pasos para el aprovisionamiento","text":"<ol> <li> <p>Cargar nuestra propia configuraci\u00f3n de Vagrant en <code>config_vms.yaml</code></p> </li> <li> <p>Detalles a tener en cuenta:</p> <ul> <li>Dependiendo de la cantidad de m\u00e1quinas virtuales que requieras para tu laboratorio deber\u00e1s cambiar la variable <code>vms</code> al n\u00famero correspondiente.</li> <li>Observar que la variable de <code>vb_memory</code> es igual a <code>2048</code> , sino habr\u00e1 problemas con la ejecuci\u00f3n de cualquier configuraci\u00f3n de Kubernetes debido a que el requerimiento minimo de memoria son 2GB.</li> <li>Recordar que debes cambiar las variables <code>pub_key_path</code> y <code>priv_key_path</code> con los valores correspondientes a la ruta hacia tus claves p\u00fablica y privada respectivamente.</li> <li>Recordar que debes cambiar la variable <code>base_pub_ip</code> a la correspondiente a tu red LAN del laboratorio u hogar.</li> <li>Recordar que debes cambiar la variable <code>bridged_iface</code> por la interfaz correspondiente a la que est\u00e1 conectada a tu red LAN del laboratorio u hogar.</li> </ul> <p>Deber\u00e1s colocar en la variable <code>env</code> la correspondiente a tu configuraci\u00f3n, quedando como sigue:</p> <pre><code>---\nvagrant_config:\nenv: 'tu_usuario'\nusers:\n    tu_usuario:\n      base_name: \"k8s\"\n      base_image: \"bento/ubuntu-22.04\"\n      base_image_version: \"202309.08.0\"\n      #### -&gt; Resto de tus configuraciones\n</code></pre> </li> </ol> <p>Ahora s\u00ed, podemos levantar las m\u00e1quinas virtuales estando en el directorio correspondiente al archivo <code>Vagrantfile</code>:</p> <pre><code>vagrant up\n</code></pre> <p>Para la eliminaci\u00f3n de las m\u00e1quianas creadas usar:</p> <pre><code>vagrant destroy\n</code></pre> <ol> <li>Entender el directory layout de <code>ansible/</code>:</li> </ol> <p>Existen \"buenas pr\u00e1cticas\" a la hora de acomodar los archivos que utilizamos para aprovisionar con Ansible. Pod\u00e9s encontrar m\u00e1s informaci\u00f3n ac\u00e1: Ansible Best Practicas - Directory Layout</p> <p>Luego de seguir a \u00e9ste se acomodaron los directorios y archivos de la siguiente manera:</p> <pre><code>&gt; tree\n.\n\u251c\u2500\u2500 cluster_reset.yml\n\u251c\u2500\u2500 cluster_setup.yml\n\u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 all.yml\n\u251c\u2500\u2500 host_vars\n\u251c\u2500\u2500 inventory.yml\n\u251c\u2500\u2500 old_files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-get-join-command.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-hosts.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-init-cluster.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-install-kubernetes-dependencies.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ansible-join-workers.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ansible-vars.yml\n\u251c\u2500\u2500 reset.yml\n\u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 get_join_command\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 init_cluster\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 install_kubernetes_dependencies\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 join_workers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reset\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2514\u2500\u2500 site.yml\n</code></pre> <p>Cada uno de estos directorios y archivos juega un papel importante en la organizaci\u00f3n de las configuraciones y tareas de Ansible. Los elementos principales de la estructura de directorios son:</p> <ul> <li> <p><code>cluster_reset.yml</code> y <code>cluster_setup.yml</code>: Son los puntos de entrada para los playbooks de Ansible. Estos archivos especifican las tareas que se deben realizar en el entorno objetivo.</p> </li> <li> <p><code>group_vars/all.yml</code>: Ac\u00e1 se definen variables espec\u00edficas de grupo que se aplicar\u00e1n a todos los hosts en el inventario.</p> </li> <li> <p><code>host_vars/</code>: Este directorio se utilizar\u00eda para almacenar variables espec\u00edficas de host si es necesario.</p> </li> <li> <p><code>inventory.yml</code>: Este archivo es donde se define nuestro inventario, es decir, la lista de hosts en los que Ansible ejecutar\u00e1 las tareas. Puedes definir grupos de hosts y asignar variables en este archivo.</p> </li> <li> <p><code>old_files/</code>: Este directorio contiene archivos antiguos o archivos de configuraci\u00f3n anteriores que ya no se utilizan en el proyecto, es decir, los originales previos a la reestructuraci\u00f3n.</p> </li> <li> <p><code>reset.yml</code>: Un playbook para restablecer la configuraci\u00f3n de tu sistema o cl\u00faster.</p> </li> <li> <p><code>roles/</code>: Este directorio contiene los roles de Ansible, que son m\u00f3dulos reutilizables que definen tareas espec\u00edficas. Cada rol tiene subdirectorios para las tareas y las variables por defecto asociadas a ese rol.</p> <ul> <li> <p><code>get_join_command/</code>, <code>init_cluster/</code>, <code>install_kubernetes_dependencies/</code>, <code>join_workers/</code>, y <code>reset/</code> son los nombres de los roles que est\u00e1s utilizando en tu proyecto.</p> </li> <li> <p>Dentro de cada rol, hay subdirectorios <code>defaults</code> y <code>tasks</code> que contienen variables por defecto y tareas espec\u00edficas para ese rol.</p> </li> </ul> </li> <li> <p><code>site.yml</code>: Es un archivo de nivel superior que suele utilizarse para orquestar la ejecuci\u00f3n de varios playbooks y roles en un orden espec\u00edfico.</p> </li> </ul> <p>Notar\u00e1s que como extra tenemos el playbook y role para hacer un reset de las configuraciones, entonces no ser\u00e1 necesario eliminar todas las m\u00e1quinas virtuales para volver a comenzar en caso de error.</p> <ol> <li> <p>Comprender lo que hace cada role en <code>roles</code>:</p> </li> <li> <p><code>install_kubernetes_dependencies</code>:</p> <ul> <li>Instala paquetes necesarios para configurar Kubernetes y Docker, como <code>apt-transport-https</code>, <code>docker-ce</code>, <code>kubelet</code>, etc.</li> <li>Configura claves de firma y repositorios para Docker y Kubernetes.</li> <li>Asegura que Docker est\u00e9 habilitado y en funcionamiento.</li> <li>Deshabilita el archivo de swap y elimina las configuraciones de swap.</li> <li>Reinicia el sistema para aplicar los cambios.</li> </ul> </li> <li> <p><code>init_cluster</code>:</p> <ul> <li>Configura Docker para usar el controlador de cgroups systemd.</li> <li>Inicializa el cl\u00faster de Kubernetes con un comando <code>kubeadm init</code>, especificando una m\u00e1scara de subred para la red de pod.</li> <li>Crea un directorio <code>.kube</code> en el directorio de inicio del usuario.</li> <li>Configura el archivo de configuraci\u00f3n de Kubernetes en el directorio de inicio del usuario.</li> <li>Reinicia el servicio kubelet para aplicar las configuraciones.</li> <li>Descarga y aplica las configuraciones de red Calico y el panel de control de Kubernetes Dashboard.</li> </ul> </li> <li> <p><code>get_join_command</code>:</p> <ul> <li>Extrae el comando para unirse al cl\u00faster Kubernetes con el comando <code>kubeadm token create --print-join-command</code>.</li> <li>Guarda el comando de uni\u00f3n en un archivo local (<code>join_command.out</code>).</li> </ul> </li> <li> <p><code>join_workers</code>:</p> <ul> <li>Configura Docker para usar el controlador de cgroups systemd.</li> <li>Lee el comando de uni\u00f3n del archivo local.</li> <li>Ejecuta el comando de uni\u00f3n para agregar nodos trabajadores al cl\u00faster.</li> </ul> </li> <li> <p><code>reset</code>:</p> <ul> <li>Elimina los paquetes de Kubernetes y Docker instalados previamente.</li> <li>Elimina las claves de firma y los repositorios relacionados con Docker y Kubernetes.</li> <li>Elimina cualquier configuraci\u00f3n de intercambio y habilita el intercambio si estaba deshabilitado previamente.</li> <li>Elimina la configuraci\u00f3n del controlador de cgroups Docker.</li> <li>Reinicia el sistema para aplicar los cambios.</li> </ul> </li> <li> <p>Modificar la variable <code>ansible_private_key: tu_ruta/tu_clave_privada</code> del archivo <code>group_vars/all.yml</code>.</p> </li> <li> <p>Checkear que se hayan levantado correctamente las m\u00e1quinas virtuales:</p> </li> </ol> <pre><code>&gt; vagrant status\nCurrent machine states:\n\nk8s-1                     running (virtualbox)\nk8s-2                     running (virtualbox)\nk8s-3                     running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n</code></pre> <ol> <li>\u00a1Atenci\u00f3n! Seg\u00fan tus requerimientos en cuanto a cantidad de m\u00e1quinas virtuales (cual n\u00famero definiste en la variable <code>vms</code> del archivo de configuraci\u00f3n <code>config_vms.yaml</code>) deber\u00e1s modificar el inventario:</li> </ol> <pre><code>---\n\nall:\nchildren:\n   kube_server:\n      hosts:\n      k8s-1:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 1 }}\"\n   kube_agents:\n      hosts:\n      k8s-2:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 2 }}\"\n      k8s-3:\n         ansible_host: \"{{ foo.base_host_ip }}.{{ foo.start_host_ip + 3 }}\"\n</code></pre> <p>En nuestro caso, tenemos 3 m\u00e1quinas virtuales, en el caso de haber m\u00e1s o menos nos aseguraremos de agregarla o eliminarla seg\u00fan corresponda.</p> <ol> <li>Comprobamos conectividad con todos los nodos:</li> </ol> <pre><code># En el directorio /ansible\nansible -i inventory.yml -m ping all\n</code></pre> <p>Obtendr\u00edamos el siguiente output:</p> <pre><code>k8s-1 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\nk8s-2 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\nk8s-3 | SUCCESS =&gt; {\n   \"ansible_facts\": {\n      \"discovered_interpreter_python\": \"/usr/bin/python3\"\n   },\n   \"changed\": false,\n   \"ping\": \"pong\"\n}\n</code></pre> <ol> <li>Ejecutar el aprovisionamiento con Ansible:</li> </ol> <pre><code># En el directorio /ansible\nansible-playbook -vvv site.yml -i inventory.yml\n</code></pre> <p>En caso de querer resetear la configuraci\u00f3n:</p> <pre><code># 1) Ejecutamos el role\nansible-playbook reset.yml -i inventory.yml\n\n# 2) Eliminamos el archivo creado con los commands\nrm  rm join_command.out \n</code></pre> <p>En caso de querer ejecutar s\u00f3lo un role espec\u00edfico</p> <pre><code>ansible-playbook -vvv site.yml -i inventory.yml --tags tag_del_rol_a_ejecutar\n</code></pre> <ol> <li>Checkear que todo se haya ejecutado correctamente:</li> </ol> <p>Primero, veremos que el output luego de ejecutar el playbook de Ansible es el siguiente:</p> <pre><code>PLAY RECAP **********************************************************************************************\nk8s-1                      : ok=31   changed=21   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nk8s-2                      : ok=21   changed=12   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nk8s-3                      : ok=21   changed=16   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n</code></pre> <p>Segundo, ingresamos por SSH a la m\u00e1quina que hayamos definido como control-plane o master y nos fijaremos los nodos:</p> <pre><code>&gt; ssh -i ~/.ssh/vagrant_key vagrant@192.168.55.51\nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-83-generic x86_64)\n\n* Documentation:  https://help.ubuntu.com\n* Management:     https://landscape.canonical.com\n* Support:        https://ubuntu.com/advantage\n\nSystem information as of Thu Nov  2 05:33:07 PM UTC 2023\n\nSystem load:  1.34521484375      Users logged in:          0\nUsage of /:   15.6% of 30.34GB   IPv4 address for docker0: 172.17.0.1\nMemory usage: 12%                IPv4 address for eth0:    10.0.2.15\nSwap usage:   0%                 IPv4 address for eth1:    192.168.102.51\nProcesses:    166                IPv4 address for eth2:    192.168.55.51\n\n\nThis system is built by the Bento project by Chef Software\nMore information can be found at https://github.com/chef/bento\nLast login: Thu Nov  2 17:34:59 2023 from 192.168.55.1\n\nvagrant@k8s-1:~$ kubectl get nodes\nNAME    STATUS   ROLES                  AGE   VERSION\nk8s-1   Ready    control-plane,master   2m    v1.23.6\nk8s-2   Ready    &lt;none&gt;                 96s   v1.23.6\nk8s-3   Ready    &lt;none&gt;                 96s   v1.23.6\n</code></pre> <p>\u00a1Listo! Tenemos nuestro peque\u00f1o cluster de Kubernetes levantado en nuestro entorno de laboratorio.</p>"},{"location":"09_attachments/#instalacion-de-kubeflow","title":"Instalaci\u00f3n de Kubeflow","text":"<p>Para instalar Kubeflow necesitamos:</p> <ul> <li>Aprovisionamiento de infraestructura: Creaci\u00f3n de los nodos con Terraform o Vagrant.</li> <li>Aprovisionamiento de Sofware: Configuraci\u00f3n de los nodos e instalaci\u00f3n de Kubernetes (k8s)</li> <li>Instalaci\u00f3n manual de Kubeflow: Usando los manifests que proporcionan en su repositorio.</li> </ul>"},{"location":"09_attachments/#aprovisionamiento-de-infraestructura-con-terraform-o-vagrant","title":"Aprovisionamiento de infraestructura con Terraform o Vagrant","text":"<p>Tanto como para Vagrant como para Terraform tendremos en cuenta un archivo de configuraci\u00f3n principal, <code>k8s/ansible/group_vars/all.yml</code>. Dentro del archivo deberemos crear el perfil para nuestra prueba, donde modificaremos diferentes par\u00e1metros. En un principio, copiaremos debajo de las existentes y dentro de los usuarios un nuevo usuario con el nombre de nuestra preferencia, quedando con la siguiente forma:</p> <pre><code>---\nsettings:\nenv: '&lt;nombre-de-nuestro-perfil&gt;'\nusers:  \n    &lt;nombre-de-nuestro-perfil&gt;:\n        prod_test: false # Si es Vagrant, false, sino true, esto deshabilita o habilita los reinicios de las VPCs respectivamente para evitar errores.\n\n        environment: \"\" # Variables de entorno que quisieramos agregar a Kubelet\n\n        user_dir_path: /home/aagustin # Path al home del local-host\n        node_home_dir: /home/vagrant # Path al home del remote-host\n\n        shared_folders:\n            - host_path: ./shared_folder # Para Vagrant, indicamos un path respecto a la Vagrantfile del local-host\n            vm_path: /home/vagrant # Para Vagrant, indicamos un path donde querramos compartir con el local-host\n\n        cluster_name: Kubernetes Cluster # Para Vagrant, indica el nombre del grupo de VPC's que se va a crear (es visualizable abriendo VirtualBox)\n\n        ssh:\n            user: \"vagrant\" # Usuario de SSH configurado en el remote-host\n            password: \"vagrant\" # Clave de SSH configurada en el remote-host\n            private_key_path: /home/aagustin/.ssh/vagrant_key # Path a la clave SSH privada guardada en el local-host\n            public_key_path: /home/aagustin/.ssh/vagrant_key.pub # Path a la clave SSH p\u00fablica guardada en el local-host\n\n        nodes:\n            control:\n                cpu: 4 # Para Vagrant, cores asignados al master\n                memory: 4096 # Para Vagrant, memoria asignada al master\n            workers:\n                count: 2 # Configurar cantidad de Workers\n                cpu: 2 # Para Vagrant, cores asignados a los workers\n                memory: 4096 # Para Vagrant, memoria asignada a los workers\n\n        network:\n            control_ip: 192.168.100.171 # Configuraci\u00f3n de la IP del nodo master\n            dns_servers:\n                - 8.8.8.8 # DNS de Google, para acceso a Internet\n                - 1.1.1.1 # DNS de Cloudflare, para acceso a Internet\n            pod_cidr: 172.16.1.0/16 # No tocar, pool de IP para los pods\n            service_cidr: 172.17.1.0/18 # No tocar, pool de IP para los servicios\n\n\n        software:\n            box: bento/ubuntu-22.04 # Para Vagrant, imagen a utlizar\n            calico: 3.25.0 # Versi\u00f3n de Calico para configurar la red de los Pods\n            kubernetes: 1.26.1-00 # Versi\u00f3n de Kubernetes para instalarlo y configurar CRI-O\n            os: xUbuntu_22.04 # Versi\u00f3n del SO para configurar CRI-O\n            kustomize: 5.0.3 # La versi\u00f3n de Kustomize que requiere Kubeflow 1.8\n            kubeflow: 1.8 # La versi\u00f3n del repo de manifests que queremos descargar\n</code></pre> <p>\u00a1IMPORTANTE! : Recordar seleccionar en la variable <code>env</code> nuestro usuario.</p>"},{"location":"09_attachments/#vagrant_1","title":"Vagrant","text":"<p>Habiendo creado nuestro perfil, deberemos tener en cuenta de modificar los siguientes par\u00e1metros para nuestra infraestructura:</p> <ol> <li> <p>Deshabilitar los reinicios debido a problemas con carpetas compartidas:</p> <pre><code>prod_test: false # Si es Vagrant, false, sino true, esto deshabilita o habilita los reinicios de las VPCs respectivamente para evitar errores.\n</code></pre> </li> <li> <p>Configuraci\u00f3n de SSH:</p> <pre><code>ssh:\n    user: \"vagrant\" # Usuario de SSH configurado en el remote-host\n    password: \"vagrant\" # Clave de SSH configurada en el remote-host\n    private_key_path: /home/aagustin/.ssh/vagrant_key # Path a la clave SSH privada guardada en el local-host\n    public_key_path: /home/aagustin/.ssh/vagrant_key.pub # Path a la clave SSH p\u00fablica guardada en el local-host\n</code></pre> <p>\u00a1Importante! Debimos haber creado nuestra clave SSH previamente.</p> </li> <li> <p>Configuraci\u00f3n de la cantidad de recursos a asignar a los nodos y la cantidad de nodos:</p> <pre><code>nodes:\n    control:\n        cpu: 4 # Para Vagrant, cores asignados al master\n        memory: 4096 # Para Vagrant, memoria asignada al master\n    workers:\n        count: 2 # Configurar cantidad de Workers\n        cpu: 2 # Para Vagrant, cores asignados a los workers\n        memory: 4096 # Para Vagrant, memoria asignada a los workers\n</code></pre> </li> <li> <p>Configuraci\u00f3n de red:</p> <pre><code>network:\n    control_ip: 192.168.100.171 # Configuraci\u00f3n de la IP del nodo master\n</code></pre> <p>\u00a1Importante! En el caso de Vagrant, no es necesario que sea una IP de la red de nuestra LAN, debido a que se crear\u00e1 una nueva red privada para los nodos.</p> </li> <li> <p>Configuraci\u00f3n del sistema:</p> <pre><code>software:\n    box: bento/ubuntu-22.04 # Para Vagrant, imagen a utlizar\n</code></pre> </li> </ol> <p>Finalmente, podemos levantar nuestros nodos con la Vagrantfile:</p> <pre><code># Posicionados en &lt;repo-dir&gt;/kubernetes/k8s/\nvagrant up\n</code></pre> <p>En el caso de necesitar destruir las m\u00e1quinas virtuales:</p> <pre><code># Posicionados en &lt;repo-dir&gt;/kubernetes/k8s/\nvagrant destroy\n</code></pre>"},{"location":"09_attachments/#terraform","title":"Terraform","text":"<p>En nuestro caso nos encontramos aprovisionando infraestructura utilizando como base la plataforma de virtualizaci\u00f3n Proxmox, donde tendremos disponible ciertos recursos que destinaremos a la creaci\u00f3n de los nodos (m\u00e1quinas virtuales) mediante Terraform utilizando de provider justamente a Proxmox.</p> <p>Adem\u00e1s de modificar el archivo de <code>k8s/ansible/group_vars/all.yml</code>, deberemos modificar nuestros archivos de <code>&lt;project-dir&gt;/terraform/</code>.</p> <p>Comenzaremos modificando los valores de los archivos de <code>&lt;project-dir&gt;/terraform/</code>:</p> <ol> <li> <p>Modificamos el archivo <code>&lt;project-dir&gt;/terraform/main.tf</code>:</p> <pre><code>terraform {\nrequired_providers {\n    proxmox = {\n    source  = \"telmate/proxmox\" # Seleccionamos el provider de proxmox\n    version = \"2.9.11\"\n    }\n}\n}\n\nprovider \"proxmox\" {\n\npm_debug = true\npm_api_url = \"https://192.168.100.100:8006/api2/json\" # \npm_api_token_id = \"terraformuser@pam!terraformuser_token\" # Usuario Proxmox hardcodeado\npm_api_token_secret = \"...\" # Token de proxmox hardcodeado\npm_tls_insecure = true\npm_log_levels = {\n    _default    = \"debug\"\n    _capturelog = \"\"\n    }\n}\n\n\nresource \"proxmox_vm_qemu\" \"vms-pps\" {\n\ncount       = length(var.proxmox_nodes)\nname        = \"k8spps${count.index+1}\" # Modificamos el nombre de nuestras vm's\ndesc        = \"k8s pps\" # Modificamos la descripci\u00f3n de nuestras vm's\nvmid      = \"70${count.index+1}\" # Modificamos el ID de nuestras vm's\ntarget_node = var.proxmox_nodes[count.index] # Crear\u00e1 los nodos seg\u00fan la lista en el archivo 'vars.tf'\nclone       = var.template_name\nagent       = 1\nos_type     = \"cloud-init\"\ncores       = 8 # Modificamos la cantidad de n\u00facleos de nuestras vm's\nsockets     = 1\ncpu         = \"host\"\nmemory      = 8192  # Modificamos la cantidad de memoria de nuestras vm's\nonboot      = true\nscsihw      = \"virtio-scsi-single\"\nbootdisk    = \"scsi0\"\n\ndisk {\n    size     = \"20G\" # Modificamos la cantidad de almacenamiento de nuestras vm's\n    type     = \"scsi\"\n    storage  = \"local-lvm\"\n    iothread = 1\n}\n\nnetwork {\n    model  = \"virtio\"\n    bridge = \"vmbr0\"\n}\n\nlifecycle {\n    ignore_changes = [\n    network,\n    ]\n}\n\nipconfig0   = \"ip=192.168.100.17${count.index+1}/24,gw=192.168.100.1\" # Modificamos las IP's de nuestras vm's\nnameserver  = \"192.168.100.1\" # Modificamos el GW de nuestras vm's\n\n}\n</code></pre> </li> <li> <p>Modificamos el archivo <code>&lt;project-dir&gt;/terraform/vars.tf</code>:</p> <pre><code>variable \"ssh_key\" {\ndefault = \"ssh-rsa ...\" # Copiamos nuestra clave privada SSH\n}\n\nvariable \"proxmox_nodes\" {\ntype    = list(string)\ndefault = [\"controlador\", \"nodo1\", \"nodo2\"] # Le damos un nombre a cada nodo y definimos la cantidad a\u00f1adiendo o quitando elementos a esta lista\n}\n\nvariable \"template_name\" {\n    default = \"ubuntu-2204-template-labredes-pass-key-sudoer-nopasswd\" # Elegimos la template a utilizar\n}\n</code></pre> </li> <li> <p>El archivo <code>&lt;project-dir&gt;/terraform/create_template.sh</code> nos permite hacer modificaciones en las mismas m\u00e1quinas virtuales durante su creaci\u00f3n, es un conjunto de comandos que nos permitir\u00e1, por ejemplo, darle permisos de super-usuario al usuario o inyectarle las claves p\u00fablicas SSH a los known-host. Modificaremos este archivo en caso de que cambiemos de cluster o movamos de lugar las claves SSH, las nombremos de manera distinta o necesitemos cambiar el nombre de la carpeta del usuario. Las l\u00edneas que deberemos modificar en este caso son las siguientes:</p> <pre><code>sudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'useradd -m -s /bin/bash labredes' # Para a\u00f1adir el usuario \"labredes\"\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'echo \"labredes:labredes\" | chpasswd' # Para a\u00f1adirle la contrase\u00f1a \"labredes\" al usuario \"labredes\"\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'usermod -aG sudo,adm labredes' # Para darle permisos de administrador y super-usuario al usuario \"labredes\"\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'mkdir -p /home/labredes/.ssh' # Para crear la carperta del usuario en home y la carpeta .ssh\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --ssh-inject labredes:file:/root/.ssh/id_key_labredes.pub # Para inyectar la clave p\u00fablica\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'chown -R labredes:labredes /home/labredes/.ssh' # Para cambiar la propiedad de la carpeta home al usuario \"labredes\"\nsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'echo \"labredes ALL=(ALL) NOPASSWD: ALL\" &gt;&gt; /etc/sudoers' # Para permitir al usuario \"labredes\" ejecutar comandos sudo sin escribir 'sudo &lt;command&gt;'\n</code></pre> </li> </ol> <p>Finalmente, aplicamos los siguientes comandos de Terraform:</p> <p>a. Para inicializar un directorio de trabajo de Terraform. Descargar y configurar los proveedores de infraestructura necesarios, as\u00ed como cualquier m\u00f3dulo de Terraform que est\u00e9 siendo utilizado. Es el primer comando que se debe ejecutar al trabajar con un nuevo proyecto de Terraform.</p> <pre><code># Posicionados en &lt;project-dir&gt;/terraform/\nterraform init\n</code></pre> <p>b. Para crear un plan de ejecuci\u00f3n detallado de los cambios que se aplicar\u00e1n a la infraestructura. Examinar los archivos de configuraci\u00f3n de Terraform y determinar qu\u00e9 recursos se crear\u00e1n, modificar\u00e1n o eliminar\u00e1n. El plan tambi\u00e9n muestra los valores de los atributos de los recursos y cualquier cambio propuesto.</p> <pre><code># Posicionados en &lt;project-dir&gt;/terraform/\nterraform plan\n</code></pre> <p>c. Para aplicar los cambios definidos en el archivo de configuraci\u00f3n de Terraform y realizar las acciones necesarias para lograr el estado deseado de la infraestructura. Terraform leer\u00e1 el plan generado por el comando terraform plan y solicitar\u00e1 confirmaci\u00f3n antes de aplicar los cambios. Una vez confirmado, Terraform crear\u00e1, modificar\u00e1 o eliminar\u00e1 los recursos seg\u00fan lo especificado.</p> <pre><code># Posicionados en &lt;project-dir&gt;/terraform/\nterraform apply\n</code></pre> <p>Ahora, modificando los valores de <code>k8s/ansible/group_vars/all.yml</code>:</p> <ol> <li> <p>Habilitar los reinicios de las VPCs:</p> <pre><code>prod_test: true # Si es Vagrant, false, sino true, esto deshabilita o habilita los reinicios de las VPCs respectivamente para evitar errores.\n</code></pre> </li> <li> <p>Configuraci\u00f3n de SSH:</p> <pre><code>ssh:\n    user: \"labredes\" # Usuario de SSH configurado en el remote-host\n    password: \"labredes\" # Clave de SSH configurada en el remote-host\n    private_key_path: /home/aagustin/.ssh/cluster_key # Path a la clave SSH privada guardada en el local-host\n    public_key_path: /home/aagustin/.ssh/cluster_key.pub # Path a la clave SSH p\u00fablica guardada en el local-host\n</code></pre> <p>\u00a1Importante! Debimos haber creado nuestra clave SSH previamente.</p> </li> <li> <p>Aqu\u00ed solo deberemos modificar la cantidad de nodos (sin borrar nada de lo otro):</p> <pre><code>nodes:\n    workers:\n        count: 2 # Configurar cantidad de Workers\n</code></pre> </li> <li> <p>Configuraci\u00f3n de red:</p> <pre><code>network:\n    control_ip: 192.168.100.171 # Configuraci\u00f3n de la IP del nodo master\n</code></pre> </li> </ol>"},{"location":"09_attachments/#aprovisionamiento-de-software-con-ansible","title":"Aprovisionamiento de software con Ansible","text":"<p>Aqu\u00ed simplemente modificamos, tanto como si utilizamos Vagrant o Terraform, los siguientes valores de <code>k8s/ansible/group_vars/all.yml</code>:</p> <ol> <li> <p>Seleccionamos las versiones de los diferentes elementos:</p> <p><code>yml    software:     calico: 3.25.0 # Versi\u00f3n de Calico para configurar la red de los Pods     kubernetes: 1.26.1-00 # Versi\u00f3n de Kubernetes para instalarlo y configurar CRI-O     os: xUbuntu_22.04 # Versi\u00f3n del SO para configurar CRI-O     kustomize: 5.0.3 # La versi\u00f3n de Kustomize que requiere Kubeflow 1.8     kubeflow: 1.8 # La versi\u00f3n del repo de manifests que queremos descargar</code></p> </li> </ol> <p>Adem\u00e1s deberemos modificar el inventario en ambos casos, en nuestro caso, para ser p\u0155acticos separamos en dos inventarios correspondientes a las prubas locales (<code>ansible/inventory_local.yml</code>) y las pruebas de laboratorio (<code>ansible/inventory_lab.yml</code>). Modificaremos el que corresponda como sigue:</p> <pre><code>---\n\nall:\n  children:\n    kube_master:\n      hosts:\n        master-node-171:\n          ansible_host: \"{{ CONTROL_IP }}\"\n    kube_workers:\n      hosts:\n        worker-node-172:\n            ansible_host: \"{{ IP_SECTIONS }}172\"\n        worker-node-173:\n            ansible_host: \"{{ IP_SECTIONS }}173\"\n        ...\n        ...\n        worker-node-17N:\n            ansible_host: \"{{ IP_SECTIONS }}17N\"\n</code></pre> <p>\u00a1Importante! Como vemos, a\u00f1adiremos tantos worker-node's como hayamos creado en la secci\u00f3n de infraestructura y deberemos asignar manualmente la IP de HOST correspondiente a cada uno.</p> <p>Comprobamos conexi\u00f3n con los nodos con el m\u00f3dulo <code>ping</code>:</p> <pre><code># Posicionados en &lt;repo-dir&gt;/kubernetes/k8s/\nansible -i ansible/inventory_&lt;local o lab&gt;.yml -m ping all\n</code></pre> <p>Deber\u00edamos ver PING con respuesta PONG de cada uno de los nodos que hayamos creado.</p> <p>Finalmente, para correr hacer el aprovisionamiento de Software ejecutamos el siguiente comando:</p> <pre><code># Posicionados en &lt;repo-dir&gt;/kubernetes/k8s/\nansible-playbook -vvv ansible/site.yml -i ansible/inventory_&lt;local o lab&gt;.yml\n</code></pre> <p>-vvv: Indica el nivel de Verbose (logs) que veremos, podr\u00edamos no usar ese par\u00e1metro si no quisi\u00e9ramos demasiados logs.</p> <p>\u00a1Importante! Debemos adem\u00e1s seleccionar el inventario seg\u00fan corresponda.</p>"},{"location":"09_attachments/#instalacion-manual-de-kubeflow","title":"Instalaci\u00f3n manual de Kubeflow","text":"<p>Para la instalaci\u00f3n de Kubeflow tenemos dos m\u00e9todos: Paquetizado para diferentes plataformas o mediante los manifest (inistalaci\u00f3n manual). En nuestro caso, al hacer una instalaci\u00f3n local y limpia (bare-metal) de Kubernetes, vamos a utilizar la segunda, mediante sus manifests, los cuales se encuentran en su repositorio.</p> <p>Necesitaremos elegir la versi\u00f3n que nos convenga, en nuestro caso, utlizaremos la m\u00e1s reciente a la fecha que es la correspondiente a la branch <code>v1.8-branch</code> del correspondiente repositorio. Seleccionaremos dicha branch para observar los requerimientos.</p> <p>Leeremos el README para poder seguir el instructivo de instalaci\u00f3n, pero para tambi\u00e9n poder ver los pre-requisitos que nos solicita Kubeflow para su funcionamiento.</p> <p>\u00a1Importante! Trabajaremos en el directorio <code>~/</code> del nodo master, ingresaremos mediante SSH al mismo:</p> <pre><code>ssh -i ~/.ssh/key &lt;user&gt;@&lt;IP-master&gt;\n</code></pre> <p>Si observamos, a la fecha y para dicha versi\u00f3n nos pide:</p> <p></p> <p>El primer requisito, de la versi\u00f3n de Kubernetes, est\u00e1 cubierto debido a que hemos instalado la misma, nos falta definir una Default StorageClass.</p> <p>\u00bfQu\u00e9 es una StorageClass? Las clases de almacenamiento de Kubernetes proporcionan una forma de aprovisionar din\u00e1micamente almacenamiento persistente para aplicaciones que se ejecutan en un cl\u00faster de Kubernetes. Cada StorageClass contiene los campos provisioner, parameters y reclaimPolicy, que se utilizan cuando un PersistentVolume que pertenece a la clase debe aprovisionarse din\u00e1micamente.</p> <p></p> <p>El segundo requisito es tener Kustomize instalado, esto nos permitir\u00e1 la aplicaci\u00f3n de las configuraciones (<code>kubectl apply ...</code>) de Kubernetes de manera automatizada.</p> <p>Y por \u00faltimo, nos pide tener Kubectl, el cual est\u00e1 cubierto ya que se ha instalado durante el aprovisionamiento de Software.</p> <p>Habiendo hecho el aprovisionamiento con Ansible nos habremos asegurado de tener la Local StorageClass agregada y por defecto, de tener Kustomize instalado y de tener el repositorio correspondiente a los manifests de la versi\u00f3n deseada ya descargado, por lo que nos queda instalar manualmente Kubeflow en nuestro Cluster. Para ello tenemos dos caminos, la instalaci\u00f3n en un solo comando o la intalaci\u00f3n m\u00f3dulo a m\u00f3dulo. Elegiremos la segunda por una cuesti\u00f3n de asegurarnos la correcta instalaci\u00f3n paso a paso de cada uno de los m\u00f3dulos.</p> <ol> <li> <p>Accedemos a la carpeta de los manifests:</p> <pre><code># Posicionados en ~/\ncd manifests\n</code></pre> <p>\u00a1Importante! Se recomienda instalar comando a comando, tomando su tiempo en cada uno para checkear que se hayan levantado todos los pods, pudiendo visualizar todo esto desde el dashboard. Adem\u00e1s, puede que algunos elementos de la instalaci\u00f3n, como el Authservice no se inicien hasta que no hayamos levantado el siguiente, Dex en este caso. Por lo que se recomienda continuar si Eventos corresponden a errores de Webhooks. Ante la duda, podemos hacer la instalaci\u00f3n de un solo comando que figura en el mismo repositorio.</p> </li> <li> <p>Instalamos el cert-manager:</p> <pre><code>kustomize build common/cert-manager/cert-manager/base | kubectl apply -f -\nkubectl wait --for=condition=ready pod -l 'app in (cert-manager,webhook)' --timeout=180s -n cert-manager\nkustomize build common/cert-manager/kubeflow-issuer/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n cert-manager\n</code></pre> </li> <li> <p>Instalamos Istio:</p> <pre><code>kustomize build common/istio-1-17/istio-crds/base | kubectl apply -f -\nkustomize build common/istio-1-17/istio-namespace/base | kubectl apply -f -\nkustomize build common/istio-1-17/istio-install/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n istio-system\n</code></pre> </li> <li> <p>Instalamos el AuthService:</p> <pre><code>kustomize build common/oidc-client/oidc-authservice/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n auth\n</code></pre> </li> <li> <p>Instalamos Dex:</p> <pre><code>kustomize build common/dex/overlays/istio | kubectl apply -f -\n</code></pre> </li> <li> <p>Instalamos K-Native Serving</p> <pre><code>kustomize build common/knative/knative-serving/overlays/gateways | kubectl apply -f -\nkustomize build common/istio-1-17/cluster-local-gateway/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n knative-eventing\n</code></pre> <p>Y tambi\u00e9n:</p> <pre><code>watch kubectl get pods -n knative-serving\n</code></pre> </li> <li> <p>Creamos el namespace de Kubeflow:</p> <pre><code>kustomize build common/kubeflow-namespace/base | kubectl apply -f -\n</code></pre> </li> <li> <p>Instalamos los Kubeflow Roles:</p> <pre><code>kustomize build common/kubeflow-roles/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n kubeflow\n</code></pre> </li> <li> <p>Creamos los recursos de Istio:</p> <pre><code>kustomize build common/istio-1-17/kubeflow-istio-resources/base | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n istio-system\n</code></pre> </li> <li> <p>Creamos las Pipelines de Kubeflow</p> <pre><code>kustomize build apps/pipeline/upstream/env/cert-manager/platform-agnostic-multi-user | kubectl apply -f -\n</code></pre> <p>Checkeamos que todos los pods est\u00e9n creados y corriendo:</p> <pre><code>watch kubectl get pods -n kubeflow\n</code></pre> </li> <li> <p>Checkeamos la creaci\u00f3n de los pods del ejemplo:</p> <pre><code>watch kubectl get pods -n kubeflow-user-example-com\n</code></pre> </li> </ol>"},{"location":"09_attachments/#crear-servicio-para-exponer-el-dashboard-de-kubeflow-a-la-ip-del-nodo","title":"Crear servicio para exponer el Dashboard de Kubeflow a la IP del nodo","text":""},{"location":"09_attachments/#exponer-el-servicio-port-forward-no-recomendado","title":"Exponer el servicio (port-forward) - No recomendado","text":"<pre><code>kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n</code></pre>"},{"location":"09_attachments/#exponer-el-servicio-nodeport-recomendado","title":"Exponer el servicio (NodePort) - Recomendado","text":"<ol> <li> <p>Creamos el siguiente archivo para el servicio servicio <code>forwarding-svc.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: custom-pf-svc\nnamespace: istio-system\nspec:\ntype: NodePort\nports:\n    - targetPort: 8080 # Where the other service is listening\n    port: 80  # Where this service are available inside the cluster\n    nodePort: 30002 # Where to expose this service\nselector:\n    app: istio-ingressgateway  # Service to expose \n</code></pre> </li> <li> <p>Aplicamos la configuraci\u00f3n:</p> <pre><code>kubectl apply -f forwarding-svc.yaml\n</code></pre> </li> <li> <p>Buscamos la IP (url) donde est\u00e1 expuesto:</p> <pre><code># Caso de que no funcione el comando, ingresamos a la IP del nodo y al puerto configurado.\nkubectl get service -n istio-system custom-pf-svc --url\n</code></pre> </li> <li> <p>Ingresamos a la url que nos muestra.</p> </li> </ol>"},{"location":"09_attachments/#correr-ejemplo","title":"Correr ejemplo","text":"<p>Una vez hayamos ingresado a Kubeflow con nuestra usuario y contrase\u00f1a de ejemplo: <code>user@example.com</code> y <code>12341234</code>.</p> <p>\u00a1Importante! Como vamos a trabajar sobre HTTP y no sobre HTTPS deberemos modificar la variable de entorno de <code>APP_SECURE_COOKIES</code> y setearla en <code>false</code> en cada web app que necesitemos, en nuestro caso ser\u00e1 para Notebooks. De todas maneras no es recomendado por riesgos de seguridad. Para nuestro ejemplo:</p> <pre><code>kubectl edit deploy jupyter-web-app-deployment -n kubeflow\n</code></pre> <p>Y este tambi\u00e9n:</p> <pre><code>kubectl edit deploy volumes-web-app-deployment -n kubeflow\n</code></pre> <ol> <li> <p>Seleccionamos nuestro namespace (en nuestro caso el ejemplo que viene desplegado con la instalaci\u00f3n)</p> <p></p> </li> <li> <p>Ingresar a Kubeflow en su secci\u00f3n \"Notebooks\"</p> <p></p> </li> <li> <p>Crear un Nuevo Notebook Server haciendo clicl en \"+ New Notebook\"</p> <p></p> </li> <li> <p>Elegimos un nombre para el Notebook Server, un entorno, el tipo de im\u00e1gen, la cantidad de CPU's y RAM del mismo:</p> <p></p> </li> <li> <p>En nuestro caso no utilizamos GPU y crearemos un nuevo volumen para el mismo:</p> <p></p> </li> <li> <p>Hacemos click en \"Lauch\":</p> <p></p> </li> <li> <p>Esperamos que est\u00e9 Ready y hacemos Click en \"Connect\"</p> <p></p> </li> <li> <p>Copiamos el siguiente c\u00f3digo en el notebook:</p> <p>Basic classification: Classify images of clothing</p> <p></p> <p></p> </li> </ol> <p>\u00a1Importante! Con Ctrl + S podemos guardar el notebook creado, a partir de ac\u00e1 nos manejamos como si tuvieramos Notebook en local. Tambien recordar instalar dependencias abriendo una consola desde le mismo Notebook Server (boton + arriba a la izquierda)</p>"},{"location":"09_attachments/#extra-instalacion-local-con-minikube","title":"Extra: Instalaci\u00f3n local con Minikube","text":"<p>Si deseamos correr en local con Minikube, podemos seguir los sigueintes pasos:</p>"},{"location":"09_attachments/#instalacion-del-binario","title":"Instalaci\u00f3n del binario","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre>"},{"location":"09_attachments/#configuracion-de-alias-para-kubectl","title":"Configuraci\u00f3n de alias para kubectl","text":"<pre><code>alias kubectl=\"minikube kubectl --\"\n</code></pre>"},{"location":"09_attachments/#despliegue-de-1-nodo","title":"Despliegue de 1 nodo","text":"<pre><code>minikube start --kubernetes-version='1.26.1' --memory='12288' --cpus='8' --disk-size='80GB' --vm=true\nminikube addons enable metrics-server\n</code></pre> <p>\u00a1Importante! La cantidad de CPUs es como m\u00ednimo de 8, sino no se levantar\u00e1n todos los servicios. La RAM es como m\u00ednimo de 12HB y el almacenamiento debe ser de como m\u00ednimo de 60GB.</p>"},{"location":"09_attachments/#detener-la-ejecucion-del-cluster","title":"Detener la ejecuci\u00f3n del Cluster","text":"<pre><code>minikube stop --all\n</code></pre>"},{"location":"09_attachments/#visualizacion","title":"Visualizaci\u00f3n","text":"<p>Desde otra terminal o antes de empezar podemos correr el Dashboard con el siguiente comando:</p> <pre><code>minikube dashboard\n</code></pre>"},{"location":"09_attachments/#instalacion-de-kustomize","title":"Instalaci\u00f3n de Kustomize","text":"<ol> <li> <p>Descarga de instalador</p> <pre><code>wget https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\n</code></pre> </li> <li> <p>Instalaci\u00f3n de versi\u00f3n 5.0.3</p> <pre><code>chmod +x install_kustomize.sh\n./install_kustomize.sh 5.0.3\nchmod +x kustomize\nmv kustomize /usr/local/bin\n</code></pre> </li> </ol>"},{"location":"09_attachments/#descarga-de-los-manifiestos","title":"Descarga de los manifiestos","text":"<pre><code>git clone https://github.com/kubeflow/manifests.git -b v1.8-branch\ncd manifests/\n</code></pre>"},{"location":"09_attachments/#instalacion-de-un-solo-comando-aprox-40min","title":"Instalaci\u00f3n de un solo comando (aprox 40min)","text":"<pre><code>while ! kustomize build example | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\n</code></pre>"},{"location":"09_attachments/#exposicion-de-servicio-dentro-del-cluster-port-forward","title":"Exposici\u00f3n de servicio dentro del cluster (port-forward)","text":"<pre><code>kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n</code></pre>"},{"location":"09_attachments/#exponer-fuera-del-cluster-nodeport","title":"Exponer fuera del cluster (NodePort)","text":"<ol> <li> <p>Creamos el siguiente archivo para el servicio servicio <code>forwarding-svc.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: custom-pf-svc\nnamespace: istio-system\nspec:\ntype: NodePort\nports:\n    - targetPort: 8080 # Where the other service is listening\n    port: 80  # Where this service are available inside the cluster\n    nodePort: 30001 # Where to expose this service\nselector:\n    app: istio-ingressgateway  # Service to expose \n</code></pre> </li> <li> <p>Aplicamos la configuraci\u00f3n:</p> <pre><code>kubectl apply -f forwarding-svc.yaml\n</code></pre> </li> <li> <p>Buscamos la IP (url) donde est\u00e1 expuesto:</p> <pre><code>minikube service -n istio-system custom-pf-svc --url\n</code></pre> </li> <li> <p>Ingresamos a la url que nos muestra.</p> </li> </ol>"},{"location":"09_attachments/#configuracion-de-red","title":"Configuraci\u00f3n de Red","text":""}]}